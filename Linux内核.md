[TOC]


# Linux内核

## 3 进程管理

### 3.1 进程

> 进程：处于执行期的程序以及相关资源（打开的文件、挂起的信号、内核内部数据、处理器状态等）的总称
>
> 线程：是在进程中活动的对象，每个线程都拥有一个独立的程序计数器、进程栈和一组进程寄存器。内核调度的对象是线程，不是进程
>
> Linux不区分进程和线程，对它来说，线程只不过是一种特殊的进程而已

现代操作系统的两种**虚拟机制**：

* 虚拟处理器：给进程一种假象，让它觉得自己在独享处理器
* 虚拟内存：让进程在分配和管理内存时觉得自己拥有整个系统的内存资源

**注意**：线程之间可以共享虚拟内存，但是都拥有自己的虚拟处理器

### 3.2 进程描述

内核把进程的列表存放在叫做**任务队列**的双向循环链表中。链表中的每一项类型为`task_struct`，称为**进程描述符**的结构，描述了一个具体进程的所有信息

#### 3.2.1 分配进程描述符

Linux通过slab分配器分配`task_struct`结构，这样能够**对象复用**和**缓存着色**。

每个任务的`thread_info`结构在它的内核栈尾端分配，其中`task`域存放的是指向该任务实际`task_struct`的指针

```c
struct thread_info {
	struct task_struct *task;
    struct exec_domain *exec_domain;
    
    ...
};
```

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531162345047.png" width="600">

#### 3.2.2 进程描述符的存放

在内核中，访问任务需要获取指向`task_struct`结构的指针，通过`current`宏查找到当前进程的进程描述符，这个查找的**速度**很重要

有的硬件体系结构拿出一个专门的寄存器存放当前进程的`task_struct`指针，而有些像x86的体系结构（寄存器不太富余），就只能在**内核栈的尾部**创建`thread_info`结构，通过计算偏移量间接找到`task_struct`结构

#### 3.2.3 进程状态

进程描述符中的`state`域描述了进程的当前状态。系统中进程的状态包括：

* **TASK_RUNNING**(运行或就绪)：进程是可执行的
* **TASK_INTERRUPTIBLE**(可中断睡眠)
* **TASK_UNINTERRUPTIBLE**(不可中断睡眠)
* **__TASK_TRACED**：被其他进程跟踪的进程，例如ptrace调试的程序
* **__TASK_STOPPED**：被暂停执行的任务，通常在接收到**SIGSTOP**,**SIGTSTP**,**SIGTTIN**,**SIGTTOU**等信号时

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531162554050.png" width="700">

#### 3.2.4 设置当前状态

使用`set_task_state(task,state)`函数

#### 3.2.6 进程家族树

所有的进程都是PID为1的init进程的后代，内核在系统启动的最后阶段启动init进程，该进程读取系统的初始化脚本（initsctript）并执行其他的相关程序，最终完成系统启动的整个过程

每个`task_struct`结构都包含一个指向其父进程`task_struct`结构的`parent`指针，还包含一个`children`的子进程链表

### 3.3 进程创建

Unix将进程的创建分解到两个单独的函数中去执行：fork()和exec()

首先，fork()通过拷贝当前进程创建一个子进程，exec()负责读取可执行文件并将其载入地址空间开始运行

#### 3.3.1 写时拷贝

Linux的fork()使用**写时拷贝**页实现，是一种推迟甚至免除拷贝数据的技术，在创建子进程时，内核并不复制整个进程地址空间，而是让父子进程共享一个拷贝，只有在写入的时候，数据才会被复制。在页根本不会被写入的情况下，例如fork()之后马上exec()，进程的地址空间就不用复制了

fork()的实际开销：复制父进程的页表以及给子进程创建唯一的进程描述符

#### 3.3.2 fork()

Linux通过clone()实现fork()，clone()通过一系列参数指明父子进程需要共享的资源。fork(),vfork()和__clone()库函数都根据各自需要的参数标志去调用clone()，然后在clone()中调用do_fork()

do_fork()调用copy_process()函数，然后让进程运行。copy_process()函数的过程：

* 调用`dup_task_struct()`为进程创建一个内核栈、thread_info结构和task_struct，与父进程的值相同，此时父子进程的描述符是相同的
* 检查创建子进程后，当前用户拥有的进程数不超过分配资源限制
* 子进程开始将自己与父进程区分开：进程描述符内的很多成员清0或者初始化，大部分的数据未被修改
* 子进程状态设置为UNINTERRUPTIBAL，保证它不会投入运行
* copy_process()调用copy_flags()更新task_struct的flags成员：其中代表进程是否拥有超级用户权限的PF_SUPERPRIV标志清0，代表进程还没有调用exec()函数的PF_FORKNOEXEC的标志被设置
* 调用alloc_pid()为子进程分配PID
* 根据clone()传递进来的参数标志，copy_process()拷贝或共享打开的文件、文件系统信息、信号处理函数、进程地址空间和命名空间等。（一般这些资源会被进程的**所有线程共享**）
* 最后，copy_process()做扫尾工作并返回一个指向子进程的指针

copy_process()返回到do_fork()函数，如果copy_process()返回成功，新创建的子进程被唤醒并投入运行

**注意**：内核有意选择子进程首先执行，因为一般子进程会调用exec()函数，这样可以避免写时拷贝的额外开销

#### 3.3.3 vfork()

除了**不拷贝父进程的页表项**，vfork()和fork()的功能相同

### 3.4 线程在Linux中的实现

Linux实现线程的机制非常独特，从内核的角度来说，并没有线程这个概念，Linux把所有的**线程当做进程**来实现。线程仅仅被视为一个与其他进程共享某些资源的进程，拥有唯一隶属于自己的`task_struct`。

Windows和Sun Solaris等系统都提供了专门支持线程的机制（将线程称为**轻量级进程**），相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。而对于Linux，线程只是**进程间共享资源的一种手段**。

举例说明：对于一个包含四个线程的进程，在提供专门线程支持的系统，通常会有一个包含指向四个不同线程的指针的进程描述符，该描述符负责描述像地址空间、打开的文件等共享资源。而Linux只是创建四个进程并分配四个普通的`task_struct`结构，并指定它们共享某些资源。

#### 3.4.1 创建线程

线程的创建和普通进程类似，只是需要在调用clone()时传递一些参数标志来指明共享的资源：

```c
clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)
```

调用的结果和fork()差不多，只是父子进程**共享地址空间、文件系统资源、打开的文件描述符和信号处理程序**

传递给clone()的参数标志决定了**新创建进程的行为方式和父子进程之间共享的资源种类**，详见**P29 表3-1**

#### 3.4.2 内核线程

> 内核线程用于内核在后台执行一些任务，他们是独立运行在内核空间的标准进程
>
> 内核线程和普通进程的区别是：**内核线程没有独立的地址空间**（指向地址空间的mm指针为NULL），它们只在内核空间运行，不切换到用户空间。

例如软中断ksoftirqd和flush都是内核线程的例子

内核是通过从kthreadd内核进程衍生出所有新的内核线程，从现有内核线程创建一个新的内核线程的方法如下：

```c
struct task_struct *thread_create(int (*threadfn)(void *data), void *data, const char namefmt[], ...)
```

新的内核线程是由kthreadd进程通过clone()系统调用创建，它们将运行threadfn函数，传递的参数是data，进程命名为namefmt。

新创建的进程处于不可运行状态，需要通过wake_up_process()唤醒来运行

```c
struct task_struct *thread_run(int (*threadfn)(void *data), void *data, const char namefmt[], ...)
```

`thread_run`方法先调用`thread_run`方法，然后调用`wake_up_process()`

内核线程启动后一直运行直到调用`do_exit()`退出，或者内核的其他部分调用`kthread_stop()`退出（传递给kthread_stop()的参数是kthread_create()返回的task_struct结构地址）

```c
int kthread_stop(struct task_struct *k)
```

### 3.5 进程终结

进程终结的几种情况：

* 显式的调用`exit()`
* 隐式地在某个程序的主函数返回（C语言在main(）函数的返回点防止调用exit()的代码）
* 接收到不能处理也不能忽略的信号或者异常时，被动地终结

不管进程如何终结，大部分都是靠`do_exit()`（定义在kernel/exit.c）来完成，它的主要工作包括：

* 将task_struct的标志成员设置为PF_EXITING
* 调用`del_timer_sync()`删除任一内核定时器，确保没有定时器在排队，且没有定时器处理程序在运行
* 如果BSD记账功能开启，调用`acct_update_intergrals()`来输出记账信息
* 调用`exit_mm()`函数释放进程占用的mm_struct，如果没有别的进程在使用它（没有被共享），就彻底释放它们
* 调用`sem__exit()`函数，如果进程排队等候IPC信号，则它离开队列
* 调用`exit_files()`和`exit_fs()`来分别递减文件描述符和文件系统数据的引用计数，如果引用计数降为0，就可以直接释放
* 接着将存放在task_struct的exit_code成员中的任务退出代码置为`exit()`提供的退出代码。**退出代码存放在这里供父进程检索**
* 调用`exit_notify()`向父进程发送信号，给自己的子进程重新找**养父**（为进程组的其他线程或者init进程），并把进程的状态（task_struct中的exit_state）设置为EXIT_ZOMBIE
* 调用`schedule()`切换到新的进程，因为EXIT_ZOMBIE状态的进程不会再被调度，所以这是进程执行的最后一段代码。`do_exit()`永不返回

至此，与进程相关联的所有资源都释放掉了（假设该进程是这些资源的唯一使用者），进程不可运行且处于EXIT_ZOMBIE状态，他占用的所有内存包括**内核栈、thread_info结构和task_struct结构**。此时存在的唯一目的就是向它的父进程提供信息用于检索，父进程通知内核都是无关的信息后，进程所持有的剩余内存被释放，归还给系统使用

#### 3.5.1 删除进程描述符

在调用`do_exit()`后，进程处于僵死状态不再运行，但是系统保留了它的进程描述符，这样可以**让系统能在进程中杰后仍能获取它的信息**。可以看到，**进程终结时所做的清理工作和进程描述符的删除是分开执行的**，在父进程获得已终结的子进程信息后，通知内核它不关注这些信息后，子进程的task_struct结构被释放

回收子进程状态是通过wait()一族函数实现，他们都是通过唯一的系统调用`wait4()`来实现，它首先会挂起调用它的进程，直到有一个子进程退出，此时函数返回子进程的PID，调用**该函数提供的指针指向子进程的退出代码**

当需要释放进程描述符时，会调用`release_task()`函数，它的工作包括：

* 调用`__exit_signal()`，该函数调用`_unhash_process()`，后者再调用`detach_pid()`从pidhash上删除该进程，同时从任务列表中删除该进程
* `__exit_signal()`函数释放目前僵死进程所使用的所有剩余资源，进行最终统计和记录
* 如果这个进程是进程组的最后一个进程，且领头进程（进程组首进程）已经死掉，那么`release_task()`就通知僵死的领头进程的父进程
* `release_task()`调用`put_task_struct()`释放进程内核栈和thread_info结构所占的内存页，并释放task_struct占用的slab高速缓存

至此，进程描述符和所有进程独享的资源全部释放

#### 3.5.2 孤儿进程造成的进退维谷

当父进程在子进程之前推出时，需要保证子进程找到一个新的父进程，否则这些孤儿进程就会在退出时一直处于僵死状态。

解决方法是：给子进程在当前进程组找一个进程作为父亲，如果不行，就让init进程作为父进程

在`do_exit()`中会调用`exit_nodify()`，该函数调用`forget_original_parent()`，后者再调用`find_new_reaper()`进程寻父过程。

代码中会遍历两个链表：**子进程链表和ptrace子进程链表**，给每个子进程设置新的父进程。

**注意**：当一个进程被跟踪时，它的临时父亲被设置为调试进程，如果他们真正的父进程退出，系统会为它及其兄弟进程找一个父进程。以前的内核版本中需要遍历系统所有的进程来找到这些子进程，现在只需要遍历这个单独ptrace的子进程链表，减轻了遍历的时间消耗

## 4 进程调度

> 进程调度程序：在可运行态进程之间分配有限处理器时间资源的**内核子系统**。

### 4.1 多任务

>  多任务操作系统是同时并发地交互执行多个进程的操作系统，能使多个进程处于阻塞或者睡眠状态，这些任务位于内存中，但是并不处于可运行状态，他们利用内核阻塞自己，直到某一时间（键盘输入、网络数据等）发生。

多任务系统分为两类：

* 非抢占式多任务
* 抢占式多任务

Linux提供了抢占式的多任务模式，由调度程序决定什么时候停止一个进程的运行，以便其他进程得到运行机会，这个强制的挂起动作叫做抢占。

时间片：可运行进程在被抢占之前预先设置好的处理器时间段。

非抢占任务模式下，除非进程自己主动停止运行，否则他会一直运行。进程主动挂起自己的操作称为**让步**（yielding）

非抢占任务模式的缺点：调度程序无法对每个进程该执行多长时间做出统一规定，进程独占的CPU时间可能超出预期，另外，一个绝不做出让步的悬挂进程就能使系统崩溃

### 4.2 Linux的进程调度

2.6内核系统开发初期，为了提供对交互程序的调度性能，引入新的调度算法，最为著名的是**反转电梯最后期限调度算法**（RSDL），在2.6.3版本替代了**O(1)调度算法**，最后被称为**完全公平调度算法(CFS)**

### 4.3 策略

#### 4.3.1 I/O消耗型和CPU消耗型的进程

CPU消耗型进程把时间大多用在了执行代码上，不属于I/O驱动类型，从系统响应速度考虑，调度策略往往是降低它们的调度频率，而延长其运行时间

调度策略的主要矛盾是：**进程响应迅速和最大系统利用率（高吞吐量）**

Unix系统的调度程序更倾向于I/O消耗型程序，以提供更好的响应速度。Linux为了保证交互式应用和桌面系统的性能，对进程的响应做了优化（缩短响应时间），更倾向于调度I/O消耗型进程。

#### 4.3.2 进程优先级

调度程序总是选择时间片为用尽而且优先级最高的进程运行

Linux采用了两种不同的优先级范围：

* 第一种用nice值，范围-20~+19，默认值0；越大的nice值优先级越低。相比高nice值（低优先级）的进程，低nice值（高优先级）的进程可以获得更多的处理器时间
* 第二种是实时优先级，数值可配置，默认范围是0~99，数值越大优先级越高。任何实时进程的优先级都比普通进程高，实时优先级和nice优先级处于互不相交的范畴

#### 4.3.3 时间片

调度策略选择合适的时间片并不简单，时间片太短会增加进程切换的处理器消耗，太长会导致系统的交互响应变差

Linux的CFS调度器没有直接分配时间片到进程，它是将**处理器的使用比**划分给进程，所以进程所获得的时间片时间是和**系统负载（系统活跃的进程数）**密切相关的

Linux中新的CFS调度器，它的进程抢占时机取决于**新的可运行程序消耗了多少处理器使用比**。如果消耗的处理器使用比比当前进程小，则新进程投入运行（当前进程被强占），否则，推迟运行。

**总而言之，CFS会先根据进程的nice值预期设定每个进程的cpu使用比，而在进程调度时，需要将新的被唤醒进程实际消耗的cpu使用比和当前进程比较，如果更小，则抢占当前进程，投入运行，否则，推迟运行**

### 4.4 Linux调度算法

#### 4.4.1 调度器类

Linux调度器以模块提供，允许不同类型的进程针对性地选择调度算法，这种模块化结构成为**调度器类**，它允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。

完全公平调度(CFS)是一个针对普通进程的调度类，称为**SCHED_NORMAL**，具体算法实现定义在文件kernel/sched_fair.c中

#### 4.4.2 Unix系统中的进程调度

传统Unix系统调度：进程启动会有默认的时间片，具有高优先级的进程将运行的更频繁，而且被赋予更多的时间片。存在的问题如下：

* nice映射到时间片，就会将nice单位值对应到处理器的绝对时间，这样将会导致进程切换无法最优化进行，同时会导致进程获得的处理器时间很大程度上取决于其nice初始值。场景实例详见**P40**
* 时间片一般为系统定时器节拍的整数倍，它会随着定时器节拍改变

CFS采用的方法是：**完全摒弃时间片而是分配给进程一个处理器使用比重**，确保了调度中恒定的公平性，切换频率是在动态变化中

#### 4.4.3 公平调度

> 完美的多任务系统：每个进程获得1/*n*的处理器时间（*n*是指可运行进程的数量），同时调度给他们无限小的时间周期（交互性会很好）

CFS的做法：**允许每个进程运行一段时间、循环轮转、选择运行最少的进程作为下一个运行进程**，在所有进程总数基础上计算一个进程应该运行多久，不在依靠nice值计算绝对的时间片，而是作为**进程获得的处理器运行比的权重**，越高的nice值获得更低的处理器使用权重。

每个进程按照其权重在全部可运行进程中所占比例的“时间片”来运行，由于越小的调度周期（重新调度所有可运行进程所花的时间）交互性会越好，也就更接近完美的所任务，CFS为调度周期设定一个目标（无限小的调度周期近似值）。

当可运行任务数量区域无限大时，他们所获得的处理器使用比和时间片将趋近于0（这会增加CPU的切换消耗）。因此，CFS引入每个进程获得的时间片底线，称为最小粒度。而当进程数非常多时，由于这个最小粒度的存在，调度周期会比较长，因此CFS并非完美的多任务。

**总之，在CFS中任何进程所获得的的处理器时间是由它自己和其他所有可运行进程nice值的相对差值决定的，nice值对时间片的作用不再是算数加权，而是几何加权，CFS是近乎完美的多任务**

### 4.5  Linux调度的实现

Linux调度主要关注四个部分：

* 时间记账
* 进程选择
* 调度器入口
* 睡眠和唤醒

#### 4.5.1 时间记账

1. 调度器实体结构

   CFS不再有时间片的概念，但是它会维护每个进程运行的时间记账，需要确保每个进程在分配给它的处理器时间内运行。CFS使用**调度器实体**（文件<linux/sched.h>中的struct_sched_entity中）来追踪进程运行记账

   ```c
   struct sched_entity {
   	struct load_weight load;
       struct rb_node run_node;
       struct list_head group_node;
       ...
       u64 vruntime;
       ...
   };
   ```

   调度器实体结构作为一个名为se的成员变量，嵌入在进程描述符task_struct内

2. 虚拟实时

   vruntime变量存放进程的虚拟运行时间，这个数值的计算是经过所有可运行进程总数的标准化，以ns为单位，与定时器节拍无关

   定义在kernel/sched_fair.h文件中的update_curr()函数实现记账功能，它是系统定时器周期性调用，无论进程是在可运行态还是阻塞状态

   ```c
   static void update_curr(struct cfs_rq *cfs_rq)
   {
   	...
       __update_curr(cfs_rq, curr, delta_exec)
       ...
   }
   ```

#### 4.5.2 进程选择

**CFS**算法调度核心：**当CFS需要选择下一个运行进程时，选择具有最小vruntime的进程**

**CFS**使用红黑树组织可运行进程队列，红黑树的键值为vruntime，检索对应节点的时间复杂度为对数级别

1. 挑选下一个任务

   CFS选择进程的算法为：运行rbtree中最左边叶子结点代表的那个进程。实现的函数是`__pick_next_entity()`，定义在kernel/sched_fair.c中

   ```c
   static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)
   {
       struct rb_node *left = cfs_rq->rb_leftmost;
       
       if(!left)
           return NULL;
       
       return rb_entry(left, struct sched_entity, run_node)
   }
   ```

   **注意**：如果该函数返回值为NULL，说明树中没有任何节点，代表没有可运行进程，CFS调度器选择idle任务运行

2. 向树中加入进程

   当**进程变为可运行状态（被唤醒）或者通过fork()调用第一次创建进程时**，会将进程加入到rbtree。`enqueue_entity()`函数实现了这个过程，代码详见P45

3. 从树中删除进程

   删除动作发生在**进程阻塞（变为不可运行状态）或者终止（结束运行）**，是由函数`dequeue_entity()`函数完成     

#### 4.5.3 调度器入口

进程调度的入口函数是`schedule()`，定义在kernel/sched.c文件，**它是内核其他部分调用进程调度器的入口**。

`schedule()`通常需要和一个调度类相关联，它会先找到一个最高优先级的调度类，后者要有自己的可运行进程队列，然后这个调度类决定下一个可运行的进程。

因此，`schedule()`函数的逻辑比较简单，它的主要逻辑就是调用`pick_next_task()`，这个函数会以优先级为序，从高到低一次检查每个调度器类，从最高优先级的调度类中选择下一个运行的进程。详细代码如下图

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531163630230.png" width="600">

每个调度类都实现了`pick_next_task()`函数，它会返回指向下一个可运行进程的指针，在CFS中`pick_next_task()`会调用`pick_next_entity()`，该函数会调用 [4.5.2节](#4.5.2 进程选择) 提到的`__pick_next_entity()`

**函数优化**：由于CFS是普通进程的调度类，而系统绝大多数进程是普通进程。函数使用了一个小技巧，当所有可运行进程数等于CFS类对应的可运行进程数时，直接返回CFS调度类的下一个运行进程

#### 4.5.4 睡眠和唤醒

睡眠（或阻塞）的进程处于一个特殊的不可运行状态。

进程睡眠时，进程把自己标记为休眠状态，从可执行进程对应的红黑树中移出，放入等待队列，然后调用`schedule()`调度下一个进程；唤醒的过程相反：进程被设置成可执行状态，然后从等待队列移到可执行红黑树中

1. 等待队列

   等待队列是由**等待某些事件发生的进程组成的简单链表**，内核用`wake_queue_head_t`代表等待队列

   进程加入等待队列的详细过程和代码详见**P50**

2. 唤醒

   唤醒操作通过函数`wake_up()`进行，它会唤醒等待队列上的所有进程，它调用函数`try_to_wake_up()`将进程状态设置为**TASK_RUNNING**，调用`enqueue_task()`将此进程放入红黑树，如果被唤醒的进程比当前正在执行的进程优先级高（这里不是指nice值，而是根据CFS调度的cpu使用比规则得出的结果），还要设置进程的need_resched标志。

   **注意**：通常哪段代码促使等待条件达成，它就要负责调用`wake_up()`函数。例如，当磁盘数据到来时，VFS需要负责对等待队列调用`wake_upe()`。

如下图是每个调度程序状态之间的关系

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531163039995.png" width="600">


### 4.6 抢占和上下文切换

上下文切换由定义在kernel/sched.c中的context_switch()函数负责处理。每当一个新的进程被选出来投入运行的时候，schedule()会调用函数context_switch()，后者完成两项工作：

* 调用声明在<asm/mmu_context.h>中的switch_mm()函数，它负责将虚拟内存从上一个进程映射切换到新进程中
* 调用声明在<asm/system.h>的switch_to()，负责从上一个进程的处理器状态切换到新进程的处理器状态，其中包括**保存、恢复栈信息和寄存器信息**

内核提供一个`need_resched`标志标明是否需要重新执行一次调度，2.2以前放在全局变量，2.2~2.4在每个进程的进程描述符中（由于current宏速度很快并且进程描述符通常是在高速缓存中，访问`task_struct`内的数值比全局变量更快），而在2.6版本中，它放在thread_info结构体中，用一个特别的标志变量的一位来表示。

`need_resched`标志被设置的时机：

* 当某个进程应该被抢占时，scheduler_tick()函数会设置这个标志
* 当一个优先级更高的进程进入可运行状态时，try_to_wake_up()也会设置这个标志

然后内核检查该标志，确认被设置后，会调用schedule()切换到一个新进程

#### 4.6.1 用户抢占

内核在中断处理程序或者系统调用返回后，都会检查`need_resched`标志。从中断处理程序或者系统调用返回的返回路径都是跟体系结构相关，在entry.S(包含内核入口和退出的代码)文件通过汇编实现

当内核将返回用户空间的时候，如果`need_resched`标志被设置，会导致schedule()调用，会发生用户抢占

因此，用户抢占发生在以下情况：

* 系统调用返回用户空间时
* 中断处理程序返回用户空间时

#### 4.6.2 内核抢占

>  在没有内核抢占的系统中，调度程序没有办法在一个内核级的任务正在执行时重新调度，内核中的任务以协作方式调度，不具备抢占性，内核代码一直执行到完成（返回用户空间）或者阻塞为止

在2.6版本，Linux内核引入抢占能力，只要重新调度是**安全的**（没有持有锁的情况），内核可以在任何时间抢占正在执行的任务。

在每个进程的thread_info结构中加入preempt_count计数，代表进程使用锁的个数。

* 在中断返回内核空间的时候，会检查need_resched和preempt_count，如果need_resched被设置且preempt_count为0，则可以进行安全的抢占，调度程序schedule()会被调用，否则，中断直接返回当前进程
* 如果进程持有的所有锁被释放，preempt_count会减为0，此时释放锁的代码会检查need_resched标志，如果被设置，则调用schedule()

因此，内核抢占发生在：

* 中断处理程序正在执行，且返回内核空间之前
* 进程在内核空间释放锁的时候
* 内核任务显式的调用schedule()
* 内核中的任务阻塞

### 4.7 实时调度策略

> Linux提供了一种软实时的工作方式
>
> 软实时的定义：内核调度进程尽力使进程在规定时间到来前运行，但是内核不能总是满足这些进程的要求
>
> 硬实时的定义：保证在一定条件下，可以完全满足进程在规定的时间内完成操作

Linux提供了两种实时调度策略：SCHED_FIFO和SCHED_RR，普通的、非实时的调度策略是SCHED_NORMAL。实时策略不被CFS调度器管理，而是被一个特殊的实时调度器管理

SCHED_FIFO实现了**简单的、先入先出的调度算法**，它不使用时间片，SCHED_RR和前者大致相同，不同点在于它使用时间片，是一种**实时轮转调度算法**



## 5 系统调用

### 5.1 与内核通信

系统调用在用户空间进程和硬件设备之间添加了一个中间层，主要作用是：

* 为用户空间提供了硬件的抽象接口
* 保证了系统的稳定和安全，可以基于权限、用户类型和其他一些规则对需要进行的访问进行裁决

系统调用是用户空间访问内核的**唯一手段**，除了**异常和陷入**之外，它是内核唯一的合法入口

### 5.2 API、POSIX和C库

应用程序通过在用户空间实现的应用编程接口（API）而不是直接通过系统调用来变成，一个API定义了一组应用程序使用的编程接口，可以实现为一个或多个系统调用，或者完全不使用任何系统调用

POSIX、API、C库以及系统调用的关系如下图

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531102311595.png" width=800>

### 5.3 系统调用

#### 5.3.1 系统调用号

在Linux中，每个系统调用被赋予了一个系统调用号。

系统调用号的特点：

* 系统调用号一旦分配就不能再有变更，否则编译好的程序有可能崩溃
* 如果系统调用被删除，所占用的系统调用号不允许被回收利用，否则以前编译过的代码会调用这个系统调用，但是却调用的另一个系统调用，Linux使用“未实现”系统调用`sys_ni_syscall()`来填补这种空缺，它除了返回`-ENOSYS`外不做任何工作

系统调用表sys_call_table，为每一个有效的系统调用指定了唯一的系统调用号

#### 5.3.2 系统调用的性能

Linux系统调用比其他许多操作系统都要快，原因是：

* Linux很短的上下文切换时间，进出内核被优化的很简洁
* 系统调用处理程序和每个系统调用本身也很简洁

### 5.4 系统调用处理程序

通知内核的机制通过软中断实现：通过引发一个异常来促使系统切换到内核态去执行异常处理程序

在x86系统上预定义的软中断是中断号128，通过int $0x80指令触发中断，这条指令触发一个异常导致系统切换到内核态并执行第128号异常处理程序（这个异常处理程序就是系统调用处理程序），它的名字是system_call()

#### 5.4.1 指定恰当的系统调用

因为所有系统调用陷入内核的方式都一样，所以需要把系统调用号传给内核用于区分每种系统调用。

在x86上，系统调用号通过eax寄存器传递给内核，在陷入内核之前，用户空间把相应系统调用号放入eax中，system_call()函数将给定的系统调用号与NR_syscalls作比较来检查其有效性，如果大于或等于NR_syscalls，就返回-ENOSYS，否则，执行相应的系统调用

```asm
call *sys_call_table(, %rax, 8)
```

#### 5.4.2 参数传递

系统调用额外的参数也是存放在寄存器传递给内核。在x86-32系统上，ebx、ecx、edx、esi和edi按照顺序存放前5个参数，如果超过5个参数，需要用单独的寄存器存放**所有指向这些参数在用户空间地址的指针**

给用户空间的返回值也通过寄存机传递，在x86系统中，它存放在eax寄存器

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531110908683.png" width="800">

### 5.5 系统调用的实现

#### 5.5.1 实现系统调用

实现的几个原则：

* 尽量提供单一功能
* 系统调用应该提供标志参数以确保**向前兼容**，扩展了功能和选项
* 系统调用要考虑通用性，**提供机制而不是策略**

#### 5.5.2 参数验证

 由于系统调用在内核空间执行，为了保证系统的安全和稳定，系统调用必须仔细检查所有的参数是否合法

其中，最重要的一项就是**检查用户提供的指针是否有效**。在接收一个用户空间的指针之前，内核必须保证：

* 指针指向的内存区域属于用户空间，进程决不能哄骗内核去读内核空间的数据
* 指针指向的内存区域在进程的地址空间中，进程决不能哄骗内核去读其他进程的数据
* 内存应该标记为对应的读、写或者可执行的权限，进程决不能绕过内存访问权限

内核提供两个方法完成**必须的检查和内核空间与用户空间之间数据的来回拷贝**，分别为`copy_to_user()`和`copy_from_user()`

如果执行失败，这两个函数返回没能完成拷贝的数据的字节数；如果成功，返回0。两个方法都有可能**阻塞**，当包含用户数据的页被换出到磁盘而不再物理内存上时，就可能发生，此时进程休眠，知道缺页异常程序将改页换入物理内存

**注意**：内核无论何时都不能轻率地接受来自用户空间的指针！

**最后一项检查**：进程是否有对应系统调用的合法权限，如`reboot()`系统调用，需要确保进程拥有CAP_SYS_REBOOT功能

### 5.6 系统调用上下文

内核在执行系统调用事处于进程上下文，current指针指向当前任务，即引发系统调用的那个进程

在进程上下文中，内核可以休眠（比如在系统调用阻塞或者调用schedule()时），这说明了：

* 系统调用可以使用内核提供的绝大多数功能
* 系统调用的进程可以被其他进程抢占，新的进程可以使用相同的系统调用，因此需要保证系统调用是可重入的

#### 5.6.1 绑定一个系统调用的最后步骤

注册一个正式的系统调用的过程为：

* 在系统调用表的最后加入一个表项，从0开始递增
* 对于所支持的各种体系结构，系统调用号都必须定义于<asm/unistd.h>中
* 系统调用必须被编译进内核映像（不能被编译成模块），只需要将它放进kernel/下的相关文件就可以

例如，一个虚构的系统调用foo()注册的过程：

1. 将sys_foo加入到系统调用表中，对于大多数体系结构，该表位于entry.s文件，形式如下图，将新的系统调用`.long sys_foo`加入表的末尾

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531123204686.png" width="600">

2. 将系统调用号加入到<asm/unistd.h>，格式如下图：

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531123538872.png" width="600">

然后在该表中加入一行

```c++
#define __NR_foo 338
```

3. 最后，实现foo()系统调用，把实现代码放进kernel/sys.c文件中（asmlinkage限定词是一个编译指令，通知编译器仅从栈中提取该函数的参数）

```c
asmlinkage long sys_foo()
{
    return THREAD_SIZE
}
```

#### 5.6.2 从用户空间访问系统调用

用户程序除了通过标准头文件和C库链接来使用系统调用之外，Linux本身提供了一组宏，用于直接访问系统调用，它会设置好寄存器并调用陷入指令，这些宏的形式为_syscall*n()*（n的范围是0~6，代表传递给系统调用的参数个数）

例如，`open()`系统调用的定义是：

```c
long open(const char *filename, int flags, int mode)
```

不依靠库的支持，直接调用此系统调用的宏形式为：

```c
# define NR_open 5
_syscall3(long, open, const char*, filename, int, flags, int, mode)
```

每个宏都有2+2*n个参数，第一个参数对应系统调用的返回类型，第二个参数是系统调用的名称，之后就是系统调用参数顺序排列的每个参数的类型和名称。该宏会扩展成内嵌汇编的C函数，汇编语言执行将系统调用号压入寄存器并触发软中断陷入内核的过程

## 6 内核数据结构

## 7 中断和中断处理

## 8 下半部和推后执行的工作

## 9 网络

### 9.1 网络实现的分层模型

内核网络子系统的实现与TCP/IP模型很相似，相关的C语言代码划分为不同层次，各层次都有明确定义的任务，各个层次只能通过明确定义的接口与上下紧邻的层次通信（这样设计的优点：可以组合使用各种设备、传输机制和协议），如下图是内核对于分层结构的实现

![image-20200720194433868](D:\Tech\学习笔记\Linux学习笔记\image-20200720194433868.png)

该子系统处理了大量特定于协议的细节，穿越各层的代码路径中有大量的函数指针，没有直接的函数调用（因为各个层次存在多个组合关系）。

### 9.2 网络命名空间

### 9.3 套接字缓冲区

内核采用**套接字缓冲区**用于在网络实现的各个层次之间交换数据，无须来回复制分组数据，提高了性能

其结构定义如下

![image-20200720195417684](D:\Tech\学习笔记\Linux学习笔记\image-20200720195417684.png)
![image-20200720195445895](D:\Tech\学习笔记\Linux学习笔记\image-20200720195445895.png)

#### 9.3.1 使用套接字缓冲区管理数据

套接字缓冲区通过其中包含的各种指针与一个内存区域相关联，网络分组的数据位于该区域。

套接字缓冲区的基本思想是：通过操作指针来增删协议首部

* head和end指向数据在内存中的起始和结束位置

* data和tail指向协议数据区域的起始和结束位置

  ![image-20200720200359909](D:\Tech\学习笔记\Linux学习笔记\image-20200720200359909.png)

* mac_header指向MAC协议首部的起始，network_header和transport_header分别指向网络层和传输层协议首部的起始；在字长32位的系统上，数据类型sk_buff_data_t表示各种类型为简单指针的数据

  ```c
  typedef unsigned char *sk_buff_data_t;
  ```

* data和tail使得在不同协议层之间传递数据时，无须显式地复制操作，如下图展示了分组的合成方式

  ![image-20200721082112889](D:\Tech\学习笔记\Linux学习笔记\image-20200721082112889.png)

在一个新分组产生时，TCP层首先在**用户空间**分配内存来容纳该分组数据（首部和净荷），分配的空间大于数据实际需要的长度，因此较低的协议层可以进一步增加首部

然后分配一个套接字缓冲区，使得head和end分别指向上述内存区的起始和结束地址，而TCP数据位于data和tail之间

在套接字缓冲区传递到互联网络层时，必须增加一个新层，只需要向已经分配但尚未占用的那部分内存写入数据即可，除了data之外所有的指针都不变，data现在指向IP首部的起始处，下面的各层会重复这样的操作，直至分组完成通过网络发送

为了保证套接字缓冲区的长度尽可能小，在64位CPU上，将sk_buff_data_t改为整型变量，由于整型变量占用的内存只有指针变量的一半（前者4字节，后者8字节），该结构的长度缩减了20字节。

```c
typedef unsigned int sk_buff_data_t;
```

其中data和head仍然是常规的指针，而所有sk_buff_data_t类型的成员是前两者的偏移，如指向传输层的首部指针计算如下：

![image-20200721083726563](D:\Tech\学习笔记\Linux学习笔记\image-20200721083726563.png)

#### 9.3.2 管理套接字缓冲区数据

除了前述的指针外，套接字缓冲区还包括用于处理相关的数据和管理套接字缓冲区自身的其他成员，主要成员如下：

* tstamp：保存了分组到达的时间
* dev指定了分组的网络设备
* iif：输入设备的接口索引号
* sk：指向处理该分组套接字对应的socket实例的指针
* dst：改分组接下来通过内核网络实现的路由
* next和prev：将套接字缓冲区保存在一个双链表中（没有用内核标准链表实现，使用了手工实现的版本）
* qlen：指定了等待的长度

sk_buff_head和sk_buff的next和prev用于创建一个循环链表，套接字缓冲区的list成员指向表头，如下图

![image-20200721084613884](D:\Tech\学习笔记\Linux学习笔记\image-20200721084613884.png)

### 9.4 网络访问层

网络访问层主要负责在计算机之间传输信息，与网卡的设备驱动程序直接协作

### 9.4.1 网络设备的表示

在内核中，每个网络设备都表示为net_device结构的实例，在分配并填充该实例之后，必须用net/dev.c中的register_device函数将其注册到内核。该函数完成一些初始化任务，并将该设备注册到通用设备机制内，这会创建一个sysfs项/sys/class/net/<device>，关联到该设备对应的目录

![image-20200721085534908](D:\Tech\学习笔记\Linux学习笔记\image-20200721085534908.png)


网络设备不是全局的，是按照命名空间进行管理，每个命名空间（net实例）有如下3个实例可用：

* 所有的网络设备都保存在一个单链表中，表头为dev_base
* 按设备名散列：辅助函数dev_get_by_name(struct net *net, const char *name)根据设备名在该散列表上查找网络设备
* 按接口索引散列：辅助函数dev_get_by_index(struct net *net, int ifindex)根据给定的接口索引查找net_device的实例

net_device结构包含了与特定设备相关的所有信息，该结构非常复杂，如下图为部分成员

![image-20200721090550123](D:\Tech\学习笔记\Linux学习笔记\image-20200721090550123.png)

一些成员定义了与网络层和网络访问层相关的设备属性：

* mtu指定了一个传输帧的最大长度
* type保存了设备的硬件类型
* dev_addr存储了设备的硬件地址（如以太网的MAC地址），addr_len指向该地址的长度，broadcast是广播地址
* ip_ptr、ip6_ptr、atalk_ptr等指针指向特定于协议的数据

net_device的大多数成员都是函数指针，执行与网卡相关的典型任务，这些成员表示了与下一个协议层的抽象接口，实现同一组接口访问所有的网卡，而网卡的驱动程序负责实现细节

 #### 9.4.2 接收分组

所有现代的设备驱动程序都使用中断来通知内核有分组到达。网卡驱动程序对特定于设备的中断设置了一个处理例程，每当中断被引发时，内核都会调用中断处理程序，将数据从网卡传输到物理内存（通过DMA方式能够将数据从网卡传输到物理内存），或者通知内核在一定时间后进行处理

如下图是一个分组到达网络适配器之后，该分组穿过内核到达网络层函数的路径

![image-20200727084714720](D:\Tech\学习笔记\Linux学习笔记\image-20200727084714720.png)

分组是在中断上下文中接收的，处理例程只能执行一些基本任务，避免系统其他任务延迟太长时间

在中断上下文中，数据由3个短函数处理，执行下列任务：

* net_interrupt是设备驱动程序设置的中断处理程序，它用于确定中断是否真的由接收到的分组所引发的，如果确实如此，则控制转到net_rx
* net_rx函数也是特定于网卡，首先创建一个套接字缓冲区，并指向一块物理内存，分组的内容接下来从网卡传输到缓冲区（也就是物理内存），然后分析首部数据，确定分组数据所使用的网络层协议
* 接下来调用netif_rx，与前两个方法不同，netif_rx不是特定于网络驱动程序的，该函数位于net/core/dev.c函数中，调用该函数，标志着控制由特定于网卡的代码转到**网络层的通用接口部分**。该函数的作用在于，将接收的分组放置到一个特定于CPU的等待队列上，并退出中断上下文

内核在全局定义的softnet_data数组中管理进出分组的等待队列，数组类型为softnet_data，为提高多处理器系统的性能，对每个CPU都会创建等待队列，支持分组的并行处理。不需要显式的使用锁机制，因为每个CPU只会处理自身的队列，不会干扰其他CPU的工作

softnet_data结构如下，inpput_pkt_queue使用前面提到的sk_buff_head表头，对所有进入的分组建立一个链表

![image-20200727085803749](D:\Tech\学习笔记\Linux学习笔记\image-20200727085803749.png)

netif_rx在结束之前将软中断NET_RX_SOFTIRQ标记为即将执行，然后退出中断上下文，接下来net_rx_action用于该软中断的处理程序，其代码流程图如下（这里是简化版本）

![image-20200727090201863](D:\Tech\学习笔记\Linux学习笔记\image-20200727090201863.png)

在一些准备工作之后，工作转移到process_backlog，该函数在循环中处理下列步骤

* __skb_dequeue从等待队列中移除一个套接字缓冲区，该缓冲区管理着一个接收到的分组
* 由netif_receive_skb函数分析分组类型，以便根据分组类型将分组传递到网络层的接收函数（即传输到网络系统的更高一层），该函数遍历可能负责当前分组类型的所有网络层函数，逐个调用deliver_skb函数
* 接下来deliver_skb函数使用特定于分组类型的处理程序func，承担对分组更高一层的处理

新的协议通过dev_add_pack增加，各个数组项的类型为struct packet_type，定义如下

![image-20200729082148633](D:\Tech\学习笔记\Linux学习笔记\image-20200729082148633.png)

func是该结构的主要成员，指向网络层函数的指针，如果分组的类型合适，则将其传递给该函数

#### 9.4.3 发送分组

net/core/dev.c中的dev_queue_xmit用于将分组放置到发出分组的队列上，在分组放置到等待队列上一定时间后，分组将发出，这是由特定于适配器的函数hard_start_xmit完成，在每个net_device结构中都以函数指针出现

### 9.5 网络层

#### 9.5.1 IPv4

IP分组使用的协议首部如下图

![image-20200729083424801](D:\Tech\学习笔记\Linux学习笔记\image-20200729083424801.png)

在内核源码中，该首部由iphdr结构实现

![image-20200729083533669](D:\Tech\学习笔记\Linux学习笔记\image-20200729083533669.png)

ip_rcv函数是网络层的入口点，分组向上传过内核的路线如下图

![image-20200729083648597](D:\Tech\学习笔记\Linux学习笔记\image-20200729083648597.png)

#### 9.5.2 接收分组

在分组转发到ip_rcv（packet_type -> func）时，必须检查接收到的信息，主要是检查计算的校验和与首部中的校验和是否一致，还会检查分组是否达到了IP首部的最小长度，协议是否是IPv4（IPv6的例程是另一个）

在进行一些检查之后，内核并不立即继续对分组的处理。而是调用一个netfilter挂钩，使用户可以对分组进行操作，当内核到达一个挂钩位置时，将在用户空间调用对该标记支持的例程，接着在另一个内核函数继续内核端的处理

下一步，需要判断分组目的地是本地还是远程计算机，从而判断需要将分组转发到更高层或是转到互联网络层的输出路径上

ip_route_input负责选择路由，判断路由的结果是选择一个函数，进行进一步的处理，可用的函数分别是ip_local_deliver和ip_forward，分别对应向更高一层传递和转发到另一台计算机

#### 9.5.3 交付到本地传输层

如果分组的目的地是本地计算机，ip_local_deliver会找到一个合适的传输层函数，将分组转发过去

1.  **分片合并**

   该函数的第一项任务是通过ip_defrag重新组合分片分组的各个部分，对应的代码流程图如下

   ![image-20200729084944972](D:\Tech\学习笔记\Linux学习笔记\image-20200729084944972.png)

   内核在一个独立的缓存中管理一个分组的各个分片，改缓存称为**分片缓存**。在缓存中，属于同一个分组的各个分片保存在一个独立的等待队列中，直至改分组的所有分片都到达

   接下来调用ip_find函数，它使用基于分片ID、源地址、目标地址、分组协议标识的散列值，检查是否为对应的分组创建了等待队列。如果没有，则建立一个新的队列，并将当前处理的分组置于其上，否则返回现存队列的地址，以便ip_frag_queue将分组置于队列上

   在分组的所有分片都进入缓存，ip_frag_reasm将各个分片组合起来；如果分片尚未全部到达，ip_defrag返回NULL指针，终止互联网络层的分组处理，在所有分组都到达后，将恢复处理

2. **交付到传输层**

   接下来返回到ip_local_deliver，在分片合并完成后，调用netfilter挂钩NF_IP_LOCAL_IN，恢复在ip_local_deliver_finish函数中的处理，会根据分组的协议标识符确定一个传输层的函数，将分组传递到该函数，所有基于互联网络层的协议都有一个net_protocol结构的实例，该结构定义如下

   ```c
   struct net_protocol {
     int (*handler)(struct sk_buff *skb);
     void (*err_handler)(struct sk_buff *skb, u32 info);
      ...
   };
   ```

   * handler是协议例程
   * 在接收到ICMP错误信息并需要传递到更高层时，需要调用err_handler

   inet_add_protocol标准函数将上述结构的实例指针存储到inet_protos数组，通过散列的方法确定存储具体协议的索引位置

   在套接字缓冲区中通过指针移动“删除”IP首部后，剩下的工作就是调用传输层对应的接收例程，其函数指针存储在inet_protocol的handler字段中。例如接收TCP分组的tcp_v4_rcv例程

#### 9.5.4 分组转发

当需要将分组转发到其他计算机时，分组的目标地址分为两类：

* 目标计算机在某个本地网络中，发送计算机与该网络有连接
* 目标计算机是远程计算机，不连接本地网络，只能通过网关访问

第二种情况会复杂很多，需要找到剩余路由中的第一个站点，将分组转发到该站点，不仅需要计算机所属本地网络结构的相关信息，还需要相邻网络和相关的外出路径的信息。该信息由**路由表**提供，由内核通过多种数据结构管理，会在[9.5.5](####9.5.5 发送分组)节讨论

在接收分组时，调用的ip_route_input函数充当路由实现的接口，它能够识别出分组是交付到本地还是转发出去，同时能找到通向目标地址的路由（目标地址存储在套接字缓冲区的dst字段）

ip_forward函数的处理流程如下图

![image-20200730083703350](D:\Tech\学习笔记\Linux学习笔记\image-20200730083703350.png)

* 首先，根据TTL字段检查是否允许传输到下一跳，如果TTL<=1，则丢弃该分组，否则，将TTL减一，ip_decrease_ttl负责该工作，修改TTL的同时分组校验和也会修改
* 接下来调用netfilter挂钩NF_IP_FORWARD，之后在ip_forward_finish中恢复处理，接下来的工作委托给两个函数
  * 如果分组包含额外的选项，则在ip_forward_options函数中处理
  * dst_output函数将分组传递到在路由期间选择并保存在skb->dst->output的发送函数，通常使用ip_output，该函数将分组传递到与目标地址匹配的网络适配器

#### 9.5.5 发送分组

内核会提供通过互联网络层发送分数据的函数给较高层的协议（传输层）使用，其中ip_queue_xmit是比较常见的一个，代码流程图如下

![image-20200730085816908](D:\Tech\学习笔记\Linux学习笔记\image-20200730085816908.png)

* 首先查找可用于该分组的路由，在发送第一个分组时，内核需要查找一个新的路由（在下文讨论）
* 接下来ip_send_check为分组生成校验和
* 内核调用netfilter挂钩NF_IP_LOCAL_OUT
* 接下来调用dst_output函数，还函数基于调用skb->dst->output函数（在确定路由期间找到），后者位于套接字缓冲区中，与目标地址无关，通常该函数指向ip_output，本地产生和转发的分组将在该函数汇总

1. **转移到网络访问层**

   ip_output函数的代码流程图如下

   ![image-20200730090512169](D:\Tech\学习笔记\Linux学习笔记\image-20200730090512169.png)

   * 首先调用netfilter挂钩NF_IP_POST_ROUTING
   * 接下来是ip_finish_output
   * 如果分组长度不大于MTU，则无须分片，直接调用ip_funish_output2，该函数检查套接字缓冲区是否有足够空间容纳产生的硬件首部，如果不够，则使用skb_realloc_headroom分配额外的空间;否则调用ip_fragment实现分组的分片
   * 最后调用路由层设置的函数dst->neighbour->output，该函数指针通常指向dev_queue_xmit

2. **分组分片**

   ip_fragment将IP分组划分为更小的单位，如下图

   ![image-20200730091212221](D:\Tech\学习笔记\Linux学习笔记\image-20200730091212221.png)

3. **路由**

   每个接收到的分组分为3类：

   * 目标是本地主机
   * 目标是当前主机连接的计算机
   * 目标是远程计算机，只能经由中间系统到达

   对于第3种情况，必须根据路由选择信息来查找网关系统，分组需要通过网关发送

   内核使用散列表来加速路由的工作，路由的起始点是ip_route_input函数，它首先会在路由缓存中查找路由

   ip_route_input_slow用于根据内核的数据结构建立一个新的路由，它调用fib_lookup函数，后者的隐式返回值（一个用作参数的指针）指向一个fib_result结构的实例。fib代表信息转发库，是一个表，用于管理内核保存的路由选择信息

   路由结果关联到一个套接字缓冲区，其中的dst成员指向一个dst_entry结构的实例，该实例的内容在路由期间查找，其结构定义如下

   ![image-20200731081843643](D:\Tech\学习笔记\Linux学习笔记\image-20200731081843643.png)

   * Input和output分别用于处理进入和外出的分组，根据分组类型，会将input和output指向不同的函数:

     * 对需要交付到本地的分组，input设置为ip_local_deliver，而output设置为ip_rt_bug（该函数只会向内核日志输出一个错误信息）
  * 对于需要转发的分组，input设置为ip_forward，output设置为ip_output函数
   
* dev指定了用于处理该分组的网络设备
  
* neighbour成员存储了计算机在本地网络的IP和硬件地址，可以通过网络访问层直接到达
  
   ![image-20200731082550667](D:\Tech\学习笔记\Linux学习笔记\image-20200731082550667.png)
   
   dev保存了网络设备的数据结构而ha是设备的硬件地址，output是指向适当的内核函数的指针，在通过网络适配器传递分组时调用
   
   neighbour实例由内核中实现ARP的ARP层创建，ARP协议负责将IP地址转换为硬件地址，由于dst_dentry实例有一个成员指向neighbour，网络访问层的代码在分组通过网络适配器离开当前系统时可以调用output函数

#### 9.5.6 netfilter

netfilter是一个Linux内核框架，可以根据动态定义的条件来过滤和操作分组

1. **扩展网络功能**

   netfilter框架向内核添加了下列能力：

   * 根据状态及其他条件，对不同数据流方向（进入、外出、转发）进行**分组过滤**
   * **NAT（网络地址转换）**，根据某些规则来转换源地址和目标地址
   * 分组处理和操作，根据特定的规则拆分和修改分组

   可以在运行时向内核载入模块来增强netfilter功能，一个定义好的规则集，告诉内核在何时使用各个模块的代码

   netfilter实现由两部分组成：

   * 内核代码中的挂钩，位于网络实现的核心，用于调用netfilter代码
   * netfilter模块，其代码挂钩内核调用，但独立于其余的网络代码

   iptables由网络管理员用来配置防火墙、分组过滤器和类似功能，这些是定义在neifilter框架上的模块

2. **调用挂钩函数**

   在通过挂钩执行netfilter代码时，网络层的函数将会中断。挂钩将一个函数分为两部分，前一部分在netfilter代码调用前运行，后一部分在其后执行

   netfilter挂钩通过<netfilter.h>中的NF_HOOK宏调用，如果内核启用的netfilter支持，该宏定义如下：

   ![image-20200731090358996](D:\Tech\学习笔记\Linux学习笔记\image-20200731090358996.png)

   * pf是指调用的netfilter挂钩源自哪个协议族（IPv4层的所有调用都使用PF_INET）
   * hook是挂钩编号，如NF_IP_FOREARD和NF_IP_LOCAL_OUT，定义在<netfilter_ipv4.h>中
   * skb代表所处理的套接字缓冲区
   * indev和outdev是指向网络设备的net_device实例的指针，分别通过二者进入和离开内核（值可以为NULL）
   * okfn是一个函数指针，在netfilter挂钩结束时执行

   该宏在展开时，首先迂回到NF_HOOK_THRESH和nf_hook_thresh，然后执行nf_look_slow来处理netfilter挂钩，最后调用结束netfilter处理的okfn函数；其中的nf_hook_slow函数会遍历所有注册的netfilter挂钩并调用它们

   以IP转发为例，挂钩调用的代码如下

   ![image-20200731091121429](D:\Tech\学习笔记\Linux学习笔记\image-20200731091121429.png)
   ![image-20200731091142602](D:\Tech\学习笔记\Linux学习笔记\image-20200731091142602.png)

   其中指向的okfn是ip_forward_finish，如果没有为PF_INET和NF_IP_FORWARD注册netfilter挂钩，那么控制直接传递到该函数，否则，执行相关的netfilter代码，控制转入ip_forward_finish

3. **扫描挂钩表**

   如果注册了挂钩函数，则会调用nf_hook_slow函数，所有挂钩都保存在二维数组nf_hooks中

   ![image-20200802092331057](D:\Tech\学习笔记\Linux学习笔记\image-20200802092331057.png)

   NPPOTO指定了系统支持的协议族的最大数目（各个协议族的符号常数，例如PF_INET和PF_DECnet，保存在include/linux/socket.h），每个协议可以定义NF_MAX_HOOKS个挂钩链表，默认值是8个

   该表的list_head元素作为双链表表头，双链表可容纳nf_hooks_ops实例

   ![image-20200802092849165](D:\Tech\学习笔记\Linux学习笔记\image-20200802092849165.png)

   主要成员：

   * list：将结构连接到双链表

   * owner：指向所属模块的module数据结构的指针

   * hook：一个指向挂钩函数的指针，需要的参数和NF_HOOK宏相同

     ![image-20200802093257426](D:\Tech\学习笔记\Linux学习笔记\image-20200802093257426.png)

   * pf和hooknum指定了协议族和与挂钩相关的编号

   * 链表中的挂钩是按照优先级升序排列（比如，可以确保分组数据的处理总是在过滤器操作之前进行）

     ![image-20200802093500875](D:\Tech\学习笔记\Linux学习笔记\image-20200802093500875.png)

   可以根据协议族和挂钩编号从nf_hook数组中选择适当的链表，接下来的工作委托给of_iterate，该函数会保存所有链表元素，并调用hook函数

4. **激活挂钩函数**

   每个hook函数都返回下列值之一：

   * NF_ACCEPT：表示接受分组
   * NF_STOLEN：表示挂钩函数“窃取”了一个分组并处理该分组，此时分组已与内核无关，不必在调用其他挂钩，还需要取消其他协议层的处理
   * NF_DROP：通知内核丢弃该分组，和CF_STOLEN一洋，其他挂钩和协议层也不需要处理，同时套接字缓冲区占用的内存空间可以释放，其中包含的数据可以被丢弃
   * NF_QUEUE：将分组置于一个等待队列上，以便其数据可以由用户空间代码处理，不会执行其他挂钩函数
   * NF_REPEAT：表示再次调用该挂钩

   最后，除非所有挂钩函数都返回NF_ACCEPT，否则分组不会在网络子系统进一步处理

   内核提供了一个挂钩函数的集合，它们称为iptables，用于分组的高层处理（可以使用用户工具iptables来配置）

### 9.6 传输层

#### 9.6.1 UDP

经过前面[9.5.2节](# 9.5.2 接收分组) 介绍，ip_local_deliver负责分发IP分组传输的数据内容，net/core/udp.c中的udp_rcv用于进一步处理UDP数据报，其代码流程图如下

![image-20200802095755049](D:\Tech\学习笔记\Linux学习笔记\image-20200802095755049.png)

udp_rcv函数是 \_\_udp4_lib_rcv的包装器，而后者的输入参数是一个套接字缓冲区，在确认分组未经篡改之后，调用\_\_udp4_lib_lookup查找与之匹配的监听套接字，连接参数从UDP首部中获取，其结构如下

![image-20200802100106115](D:\Tech\学习笔记\Linux学习笔记\image-20200802100106115.png)

\_\_udp4_lib_rcv用于查找与分组目标匹配的内核内部的套接字，在有某个监听进程对分组感兴趣时，在udphash全局数组中会有与分组目标端口匹配的sock结构实例，\_\_udp_lib_lookup采用散列方法查找并返回该实例；如果找不到，则向源系统发送一个“目标不可达”的消息，并丢弃分组内容

内核中有两种数据结构表示套接字，sock是到网络访问层的接口，socket是到用户空间的接口

sock结构简化版如下

![image-20200802101629272](D:\Tech\学习笔记\Linux学习笔记\image-20200802101629272.png)

在udp_rcv找到适当的sock实例后，控制转移到udp_queue_rcv_skb，然后立即调用sock_queue_rcv_skb，它会执行两个重要的操作，完成到应用层的数据交付

* 等待通过套接字交付数据的进程，会再sk_sleep等待队列上睡眠
* 调用skb_queue_tail将包含分组数据的套接字缓冲区插入到sk_receive_queue链表末端，其表头保存在sock结构中
* 调用sk_data_ready指向的函数，通知套接字有新数据到达，这会唤醒sk_sleep队列上睡眠、等待数据到达的所有进程

#### 9.6.2 TCP

下面主要讨论TCP协议的3个主要部分：连接建立、连接终止和数据流的按序传输

TCP的状态转换如下图

![image-20200803084218092](D:\Tech\学习笔记\Linux学习笔记\image-20200803084218092.png)

1. **TCP首部**

   TCP分组的首部包含了状态数据和其他连接信息，如下图所示

   ![image-20200803084315826](D:\Tech\学习笔记\Linux学习笔记\image-20200803084315826.png)

2. **接收TCP数据**

   在互联网络层处理过分组之后，tcp_v4_rcv是TCP的入口函数，其代码流程图如下

   ![image-20200803084515122](D:\Tech\学习笔记\Linux学习笔记\image-20200803084515122.png)

   系统中的每个TCP套接字都归入3个散列表之一，分别对应下列状态：

   * 完全连接的套接字
   * 等待连接（监听状态）的套接字
   * 处于建立连接过程中的套接字

   在对分组数据进行各种检查并将首部中的信息复制到套接字缓冲区的控制块之后，内核将查找等待该分组的工作委托给\_\_inet_lookup函数，它会调用两个函数，分别扫描各种散列表，其中\_\_inet_lookup_established函数寻找一个已连接的套接字，如果没有找到合适的结构，则调用inet_lookup_listener函数检查所有的监听套接字

   与UDP相比，在找到对应该连接适当的sock结构之后，工作尚未结束，必须根据连接的状态进行相应的状态迁移，tcp_v4_do_rcv是一个多路分解器，会基于套接字的状态将代码控制流划分到不同的分支

3. **被动连接建立**

   被动连接是在接收到一个连接请求的SYN分组后出发的，它的起点是tcp_v4_rcv函数，其代码流程图如下

   ![image-20200803085509088](D:\Tech\学习笔记\Linux学习笔记\image-20200803085509088.png)

   调用tcp_v4_hnd_req执行网络层中建立连接的各种初始化任务，实际的状态迁移是在tcp_rcv_state_process函数，它由一个长的switch/case语句组成，区分各种可能的套接字状态来调用适当的传输函数

   可能的套接字状态定义在一个枚举中

   ![image-20200803085745108](D:\Tech\学习笔记\Linux学习笔记\image-20200803085745108.png)
   ![image-20200803085807606](D:\Tech\学习笔记\Linux学习笔记\image-20200803085807606.png)如果套接字的状态是TCP_LISTEN，则调用tcp_v4_conn_request，该函数结束前发送确认分组，其中包含了设置的ACK标志和接受到的分组序列号，还包含新生成的序列号和SYN标志，此时服务端的套接字状态变为TCP_SYN_RECV；下一步，发送ACK分组给客户端，客户端返回确认分组，此时套接字状态由TCP_SYN_RECV变为TCP_ESTABLISHED
   
4. **主动连接建立**

   主动连接发起是，是通过用户空间应用程序调用open库函数，发出socketcall系统调用到达内核函数tcp_v4_connect，其代码流程图如下

   ![image-20200803090535044](D:\Tech\学习笔记\Linux学习笔记\image-20200803090535044.png)

   该函数开始于查找目标主机的IP路由，在产生TCP首部并将相关的值设置到套接字缓冲区之后，套接字状态从CLOSED变为SYN_SENT，接下来tcp_connect讲一个SYN分组发送到互联网络层，接下来发送到服务端。同时，会在内核创建一个定时器，确保如果在一定时间内没有接收到确认，将重新发送分组

   接下来客户端等待服务端对SYN分组的确认以及确认连接请求的SYN分组，这回通向tcp_rcv_state_process分配器，然后控制流转到tcp_rcv_synsent_state_process函数，然后套接字状态设为ESTABLISHED，同时tcp_send_ack向服务器发送一个ACK分组，完成连接建立

5. **接收分组**

   如下代码流程图是接收分组时的代码流程图，从tcp_v4_rcv函数开始

   ![image-20200803101934382](D:\Tech\学习笔记\Linux学习笔记\image-20200803101934382.png)

   在控制传递到tcp_v4_do_rcv后，在确定目标套接字的状态为TCP_ESTABLISHED之后，调用tcp_rcv_established函数，判断分组是否易于处理，如果是，在**快速路径**中处理，否则，在**低速路径**中处理

   分组符合下列条件之一，归入易于分析：

   * 分组必须只包含对上一次发送数据的确认
   * 分组必须只包含预期接收的数据

   **快速路径**：

   * 会进行分组的检查，找到更为复杂的分组返回到低速路径
   * 接下来分析分组的长度，确认分组的内容是数据还是确认
   * 快速路径并不处理ACK部分，会调用tcp_ack函数，该函数最重要的功能是分析有关连接的新信息，同时从重传队列中删除确认数据（该队列包含所有发送的分组，如果在一定时间限制内没有收到ACK确认，则需要重传）
   * 由于进入快速路径的数据是紧接着前一部分的，无须进行进一步检查，最后调用套接字中的sk_data_ready函数指针，通知用户进程数据可用

   **低速路径**：要处理更多TCP选项，其中的代码要牵涉更广泛的内容，在低速路径中，数据保护能直接发送到套接字，需要对分组选项进行复杂的检查，然后是TCP子系统的响应，不按序到达的数据放置到一个专门的等待队列上，直至形成一个连续的数据段，才能将完整的数据传递到套接字

6. **发送分组**

   TCP分组的发送，由更高层网络协议实例对tcp_sendmsg函数的调用开始，代码流程图如下

   ![image-20200803125215783](D:\Tech\学习笔记\Linux学习笔记\image-20200803125215783.png)

   * 首先，内核会等待直至连接建立，此时套接字状态是TCP_ESTABLISHED

   * 数据从用户空间进程的地址空间复制到内核空间，用于建立一个TCP分组

   * 接下来内核到达tcp_push_one，它执行下列3个任务：

     * tcp_snd_test检查目前是否可以发送数据，比如接收方是否过载积压

     * tcp_transmit使用协议族相关的af_specific->queue_xmit函数（IPv4使用ip_queue_xmit函数），将数据转发到互联网络层

     * update_send_head处理对统计量的更新，初始化发送TCP信息段的重传定时器

   发送TCP分组的过程需要满足下列需求：

     * 接收方等待队列上必须有足够的空间可用于该数据
     * 必须实现防止连接拥塞的ECN机制
     * 必须检测某一方出现失效的情况
     * TCP慢启动机制
     * 发送但未得到确认的分组，需要超时重传

7. **连接终止**

   连接终止的状态迁移在分配器函数tcp_rcv_state_process进行，代码路径可能包含tcp_rcv_established和tcp_close函数

   主动关闭的一方，会在用户进程调用close关闭连接，调用tcp_close函数

   * 如果套接字的状态为LISTEN，则将套接字的状态改为CLOSED；否则，控制转到tcp_close_state，其中调用tcp_set_state将套接字状态设置为FIN_WAIT_1，tcp_send_fin向另一方发送FIN分组
   * 收到带有ACK标志的分组，触发FIN_WAIT_1到FIN_WAIT_2的迁移（在tcp_set_state中进行）
   * 最后收到另一方发送过来的FIN分组，则将套接字的状态改为TIME_WAIT状态（之后会自动切换到CLOSED状态）

   被动关闭的一方状态迁移过程是类似的

   * 在收到第一个FIN分组时状态是TCP_ESTABLISHED，处理由tcp_rcv_established的低速路径进行，向主动关闭一方发送ACK分组，并将套接字状态改为TCP_CLOSING
   * 然后发送FIN分组，状态变为LAST_ACK，是通过调用close库函数（调用内核的rcp_close_state函数）进行的
   * 最后接收到主动关闭方发送的ACK分组，即可终止连接，通过tcp_rcv_state_process函数将套接字状态改为CLOSED（tcp_done函数处理），释放套接字占用的内存空间，并最终终止连接

### 9.7 应用层

内核与用户空间套接字之间的接口实现在C标准库中实现，使用了socketcall系统调用，它充当一个多路分解器，将各种任务分配由不同的过程执行

对程序使用的每个套接字来说，都对应于一个socket结构和sock结构的实例，二者分别充当向下（内核）和向上（用户空间）的接口

#### 9.7.1 socket数据结构

socket结构定义如下

![image-20200802103414354](D:\Tech\学习笔记\Linux学习笔记\image-20200802103414354.png)

* type：指定所用协议类型的数字标识符

* state：表示套接字的连接状态，可使用下列值

  ![image-20200802103603545](D:\Tech\学习笔记\Linux学习笔记\image-20200802103603545.png)

  这里的枚举值，与传输层协议在建立和关闭连接的状态值没有关系，他们表示与外界（用户程序）相关的一般性状态

* file：指向一个伪文件file实例的指针，用于与套接字通信

* ops：指向proto_ops结构的指针，其中包含处理套接字的特定于协议的函数

  ![image-20200802104015706](D:\Tech\学习笔记\Linux学习笔记\image-20200802104015706.png)

  其中的需要函数指针与C标准库的函数同名，因为C库函数会通过socketcall系统调用导向上述的函数指针

* sk：指向sock结构的指针，它包含了对内核有意义的附加的套接字管理数据，其中最重要的成员放置到了sock_common结构中，并将该结构嵌入到struct sock中

  ![image-20200802104445502](D:\Tech\学习笔记\Linux学习笔记\image-20200802104445502.png)

  系统的各个sock结构实例组织在一个协议相关的散列表中，skc_node用作散列表的表元，而skc_hash表示散列值

  在接收和发送数据时，需要将数据放置到包含套接字缓冲区的等待队列上（skb_receive_queue和sk_write_queue）

  每个sock结构都关联了一组回调函数，由内核引起用户程序对特定事件的关注或进行状态改变，例如前面提到的sk_data_ready函数指针，在数据到达时，将调用它指向的函数，通常是指向sock_def_readable函数

socket结构的ops成员类型为struct proto_ops，而sock的prot成员类型为struct proto，二者容易混淆，后者定义如下

![image-20200802111837958](D:\Tech\学习笔记\Linux学习笔记\image-20200802111837958.png)
![image-20200802111902259](D:\Tech\学习笔记\Linux学习笔记\image-20200802111902259.png)

  sock结构中的操作用于套接字层和传输层之间的通信，而socket结构的ops成员包含的函数指针则用于与系统调用通信

#### 9.7.2 套接字和文件

在连接建立后，用户空间使用普通的文件操作来访问套接字，这个实现是基于VFS结构，每个套接字都分配了一个inode，inode又关联到另一个与普通文件相关的结构，用于操作文件的函数保存在一个单独的指针表i_fop中

![image-20200802112524577](D:\Tech\学习笔记\Linux学习笔记\image-20200802112524577.png)

对套接字文件描述符的文件操作，可以透明的重定向到网络子系统的代码

![image-20200802112647741](D:\Tech\学习笔记\Linux学习笔记\image-20200802112647741.png)

inode和套接字的关联，是通过下列辅助结构，将对应的两个结构实例分配到内存中的连续位置

![image-20200802112815989](D:\Tech\学习笔记\Linux学习笔记\image-20200802112815989.png)

内核提供了两个宏来进行运算，SOCKET_I根据inode找到相关的socket实例，SOCK_INODE根据socket找到inode实例

#### 9.7.3 socketcall系统调用

对于套接字的部分操作，如文件的读写操作，可以通过虚拟文件系统相关系统调用进入内核，然后重定向到socket_file_ops结构的函数指针，除此之外，还需要对套接字执行其他操作，如创建套接字、bind、listen等

因此内核提供了socketcall系统调用，它充当一个分派器，将系统调用转到其他函数并传递相关参数

![image-20200802113302644](D:\Tech\学习笔记\Linux学习笔记\image-20200802113302644.png)
![image-20200802113325385](D:\Tech\学习笔记\Linux学习笔记\image-20200802113325385.png)

下列表格对应socketcall的各个“子调用”

![image-20200802113445670](D:\Tech\学习笔记\Linux学习笔记\image-20200802113445670.png)

#### 9.7.4 创建套接字

sys_socket是创建套接字的起点，代码流程图如下

![image-20200802113750230](D:\Tech\学习笔记\Linux学习笔记\image-20200802113750230.png)

* 首先，使用sock_create创建一个新的套接字数据结构，该函数调用__sock_create，其中sock_alloc为socket实例和inode实例分配内存，这使得两个对象联合起来

* 数组static net_proto_family* net_families[NPROTO]包含所有传输协议，各个数组项都提供特定于协议的初始化函数create，它会创建一个内部的sock实例

  ![image-20200802114225988](D:\Tech\学习笔记\Linux学习笔记\image-20200802114225988.png)

* map_sock_fd为套接字创建一个伪文件，然后分配一个文件描述符，将其作为系统调用的结果返回

#### 9.7.5 接收数据

使用recvfrom和recv以及文件相关的readv和read函数来接收数据，这些函数的控制流在内核的特定位置会合并，这里只讨论recv_form对应的sys_recvfrom，期待吗流程图如下

![image-20200802114732166](D:\Tech\学习笔记\Linux学习笔记\image-20200802114732166.png)

* 套接字对应的文件描述符作为参数传递到该系统调用
* fget_light根据task_struct的描述符表查找对应的file实例
* sock_from_file确定与之关联的inode，并通过SOCKET_I找到相关的套接字
* sock_recvmsg调用特定于协议的接收例程sock->ops->recvmsg（例如UDP使用udp_recvmsg，TCP使用tcp_recvmsg），其中UDP的实现过程为：
  * 如果接收队列（sock结构的receive_queue成员实现）上至少有一个分组，则移除并返回该分组
  * 如果接收队列是空的，进程使用wait_for_packet使自身睡眠，直至数据到达
  * 在新数据到达时总是调用sock结构的data_ready函数，此时进程被唤醒
* 最后，move_addr_to_user将数据从内核空间复制到用户空间

#### 9.7.6 发送数据

用户空间程序发送数据时，可以使用两个网络相关的库函数（sendto和send）或文件层的write和writev函数，他们也会在内核的某个位置合并，这里只讨论sendto对应的sys_sendto，其代码流程图如下

![image-20200802120202420](D:\Tech\学习笔记\Linux学习笔记\image-20200802120202420.png)

 ## 12 内存管理

> 内核不能像用户空间那样奢侈的使用内存，获取内存币用户空间复杂很多

### 12.1 概述

内存管理的实现涵盖：

* 内存中物理内存页的管理
* 分配大块内存的伙伴系统
* 分配较小内存的slab、slub和slob分配器
* 分配非连续内存块的vmalloc机制
* 进程的地址空间

有两种类型计算机，分别以不同的方法管理物理内存：

* UMA计算机（一致内存访问，uniform memory access）将可用内存已连续方式组织起来（可能有小的缺口）。SMP系统中的每个处理器访问各个内存块都是同样快
* NUMA计算机（非一致内存访问，non-uniform memory access），总是多处理器计算机。系统的各个CPU都有本地内存，可支持特别快速的访问，而各个处理器之间通过总线连接起来，以支持对其他CPU本地内存的访问，比访问本地内存会慢一些

![image-20200604082129145](D:\Tech\学习笔记\Linux学习笔记\image-20200604082129145.png)

**分配阶**：表示内存中页的数目（取以2为底对应阶数的数量），例如阶1分配2<sup>1</sup>=2个页帧。

### 12.2 (N)UMA模型中的内存组织

#### 12.2.1 概述

如下图为内存划分的图示，内存划分为**结点**，每个结点关联到系统的一个处理器，对应数据结构为`pg_data_t`的实例。各个结点划分为**内存域**，分别为：

* ZONE_DMA：标识适合DMA的内存域
* ZONE_DMA32：标记了使用32位地址可寻址，适合DMA的内存域，在64位系统两种DMA内存域才有区别，而在32位计算机上，本内存域是空的，长度为0M
* ZONE_NORMAL：标记了可直接映射到内核段的普通内存域，所有体系结构都会保证这一区域的存在，但是不保证该地址对应了实际的物理内存
* ZONE_HIGHMEM：标记了超出内核段的物理内存

各个内存域都关联一个数组，组织改内存域的物理内存页（页帧），每个页帧对应结构`page`的实例。各个内存节点保存在单链表中，供内核遍历

**注意**：处于性能考虑，在为进程分配内存时，内核总是试图在当前运行的CPU相关联的NUMA节点上进行，如果当前节点的内存用尽，会使用节点**备用列表**的内存（借助于zone_list），该列表包含了其他结点

![image-20200604082802402](D:\Tech\学习笔记\Linux学习笔记\image-20200604082802402.png)

#### 12.2.2 数据结构

1. 结点管理

   pg_data_t用于节点的基本元素，定义如下

   ![image-20200604084503749](D:\Tech\学习笔记\Linux学习笔记\image-20200604084503749.png)

   * node_zones：是一个数组，包含节点中各内存域的数据结构

   * node_zonelists：指定了备用结点及其内存域的列表

   * nr_zones：节点中不同内存域的数目

   * node_mem_map：指向page实例数组的指针，用于描述结点的所有物理内存页

   * bdata：指向**自举内存分配器**数据结构的实例（系统启动期间，内存管理子系统初始化之前，内核也需要内存，会使用到自举内存分配器）

   * node_start_pfn：该结点第一个页帧的逻辑编号，系统中所有结点的页帧依次编号，每个页帧的号码全局唯一（在UMA系统中总是为0，因为其中只有一个结点）

   * node_id：全局结点id，系统中NUMA结点都从0开始编号

   * pgdat_next：连接到下一个内存结点（系统中所有结点使用单链表，结尾通过空指针标记）

   * kswapd_wait：交换守护进程的等待队列，在将页帧换出结点时会用到

   * kswapd：指向负责该节点的交换守护进程的的task_struct

   * kswapd_max_order：用于也交换子系统的实现，代表需要释放的区域的长度

2. 结点状态管理

   ![image-20200604090111563](D:\Tech\学习笔记\Linux学习笔记\image-20200604090111563.png)



### 12.3 页

内核把物理页作为内核管理的基本单元，内存管理单元（MMU）是管理内存并将虚拟内存转换为物理内存的硬件，它以页为单位来管理系统中的页表

结构体struct page表示系统中的每个物理页

```c
struct page {
    unsigned long        flags,
    atomic_t             _count,
    atomic_t             _mapcount,
    unsigned long        private,
    struct address_space *mapping,
    pgoff_t              index,
    struct list_head     lru,
    void                 *virtual
};
```

* `flags`域，用来存放页的状态（包括是不是脏的，是不是锁定在内存），每一位单独表示一种状态，至少可以表示32中不同的状态
* `_count`域，存放页的引用计数，-1时内核没有引用该页，在新的内存分配中可以使用。内核调用`page_count()`检查该域，返回0表示页空闲，返回正整数表示正在使用
* 页可以由页缓存使用（此时，mapping域指向页关联的address_space对象），或者作为私有数据（`private`指向），或者作为进程页表中的映射
* `virtual`域，页的虚拟地址，当页在高端内存（不会永久映射到内核空间）中时，这个域为`NULL`

**注意**：

1. `page`结构与物理页相关，并非与虚拟页相关，它仅仅描述当前时刻在相关物理中存放的数据（由于交换等原因，关联的数据继续存在，但是和当前物理页不再关联），它对于页的描述是短暂的
2. 页的拥有者可能是用户空间进程、动态分配的内核数据，静态内核数据或者页高速缓存等

### 12.4 页表



### 12.5 区

Linux主要使用四种区：

* ZONE_DMA，其中包含的页只能进行DMA操作（直接内存访问）
* ZONE_DMA32，和ZONE_DMA类似，不同之处是只能被32位设备访问
* ZONE_NORMAL，包含能够正常映射的页
* ZONE_HIGHMEM，包含“高端内存”，其中的页不能永久地映射到内核空间

> 高端内存，由于一些体系结构的物理内存比虚拟内存大的多，为了充分利用物理内存，将物理内存中的部分区域划分为高端内存，他们不能永久地映射到内核空间，而是动态的映射
>
> 在32位x86体系中，ZONE_HIGHMEM为高于896MB的所有物理内存，其余内存为低端内存，其中ZONE_NORMAL为16MB到896MB的物理内存，ZONE_DMA为小于16MB的物理内存
>
> x86-64系统没有高端内存区

|      区      |      描述      | 物理内存 |
| :----------: | :------------: | :------: |
|   ZONE_DMA   |  DMA使用的页   |  < 16MB  |
| ZONE_NORMAL  | 正常可寻址的页 | 16~896MB |
| ZONE_HIGHMEM |  动态映射的页  | > 896MB  |

每个区使用结构体`zone`表示，具体结构如下图

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200531164328810.png" width="800">

结构体中域的说明：

* `lock`域，是一个自旋锁，防止结构被并发访问，这个域只保护结构，不保护驻留在这个区中的页
* `watermark`域，水位值，为每个内存区设置合理的内存消耗基准
* `name`域，表示区的名字，三个区的名字分别为"DMA","Normal","HighMem"	

### 12.6 物理内存的管理

#### 12.6.1 伙伴系统结构

### 12.7 获得页

* 分配2<sup>order</sup>(1<<order)个**连续**的物理内存页，返回的指针指向第一个页的page结构体

```c
struct page *alloc_pages(gfp_t gfp_mask, unsigned int order);
```

* 将指定的物理页转换为它的逻辑地址（虚拟内存地址），返回的指针指向物理页所在的逻辑地址

```c
void *page_address(struct page *page);
```

* 和`alloc_pages()`功能类似，不过它直接返回请求的第一个页的逻辑地址

```c
unsigned long __get_free_pages(gfp_t fp_mask, unsigned int order)
```

* 只分配一页的函数

```c
struct page *alloc_page(gfp_t gfp_mask);
unsigned long __get_free_page(gfp_t fp_mask)
```

#### 12.7.1 获得填充为0的页

* 分配的所有页内容全为0，返回执行逻辑地址的指针

```c
unsigned long get_zeroed_page(unsigned int gfp_mask)
```

**注意**：为了防止页中留下一般随机的垃圾信息包含一些敏感信息，一般用户空间在获取页的时候，内容最好全部填充为0

#### 12.7.2 释放页

```c
void __free_pages(struct page *page, unsigned int order);
void free_pages(unsigned long addr, unsigned int order);
void free_page(unsigned long addr)
```

**注意**：传递了错误的struct page、地址或者order参数，都可能导致系统崩溃，因为内核是完全相信自己的

### 12.8 kmalloc()

* 以字节为单位分配内存
* 可以获得以字节为单位的一块内核内存

```c
void *kmalloc(size_t size, gfp_t flags);
```

#### 12.8.1 gfp_mask标志

标志分为三类：

* 行为修饰符
* 区修饰符
* 类型修饰符

标志具体说明详见**P209**

#### 12.8.2 kfree()

* 释放由`kmalloc()`分配的内存

```c
void kfree(const void *ptr);
```

### 12.9 vmalloc()

和`kmalloc()`类似，不同之处在于`vmalloc()`分配的内存虚拟内存连续，但是物理内存不一定连续，而`kmalloc()`分配的物理内存也是连续的

`vmalloc()`正是用户空间分配内存的方式：有`malloc()`分配的内存页在进程的虚拟内存中是连续的，但是物理内存不保证连续。大多情况下，只有硬件设备才需要连续的物理内存，他们不理解什么是虚拟内存

### 12.10 slab分配器

> Linux内核提供slab层（即slab分配器），作为通用数据结构缓存层

#### 12.10.1 slab的设计

> slab层将不同的对象划分为**高速缓存组**，每个高速缓存组存放不同类型的对象，例如，分别存放进程描述符(task_struct结构的空闲链表)，索引节点对象(struct inode)

* `kmalloc()`建立在slab层之上，使用了一组通用高速缓存
* 一般slab仅仅由一页组成，每个高速缓存由多个slab组成
* 每个slab包含一些对象成员，对象指的是被缓存的数据结构
* slab包含三种状态：满、部分满或空

高速缓存使用结构体`kmem_cache`表示，这个结构包括三个链表：`slabs_full`, `slabs_partial`和`slabs_empty`，这些链表包含高速缓存中的所有slab

slab使用slab描述符表示，详见**P216**

```c
struct slab {
    ...
};
```

slab分配器使用`__get_free_pages()`创建新的slab

#### 12.10.2 slab分配器的接口

* 创建一个新的高速缓存
  * `name`：高速缓存的名字
  * `size`：高速缓存中每个元素的大小
  * `align`：slab内第一个对象的偏移量，用来确保在页内进行特定的对齐
  * `flags`：可选的设置项，控制高速缓存的行为，详见**P218**
  * `ctor`：高速缓存的构造函数（Linux的高速缓存不使用构造函数）
  * 返回指向高速缓存的指针
  * 函数调用可能会睡眠，不能再中断上下文使用

```c
struct kmem_cache *kmem_cache_create(const char *name, 
                                    size_t size,
                                    size_t align,
                                    unsigned long flags,
                                    void (*ctor)(void *));
```

* 撤销一个高速缓存（可能睡眠，不能再中断上下文使用）

```c
int kmem_cache_destroy(struct kmem_cache *cachep);
```

**注意**：调用`kmem_cache_destroy`之前要确保两个条件： 

1. 高速缓存中的所有slab为空
2. 在调用此函数过程中不在访问这个高速缓存

* 从已经创建的缓存中分配释放对象，使用示例详见**P219**

```c
void kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);
void kmem_cache_free(struct kmem_cache *cachep, void *objp);
```

#### 12.10.3 slab分配的原理



### 12.11 栈上的静态分配

> 不同于用户栈，内核栈小而且固定，内核栈一般是两个页大小

#### 12.11.1 单页内核栈

2.6内核之后，引入选项可以设置**单页内核栈**，激活这个选项，每个进程的内核栈只有一页大小

引入的原因有两点：

1. 可以让每个进程减少内存消耗，另外随着机器运行时间增加，寻找两个连续的物理页变得越来越困难
2. 当内核栈使用两页时，中断处理程序使用它所中断进程的内核栈，而当进程使用单页的内核栈时，中断处理程序不放在进程内核栈中，而是放在**中段栈**中。

> 中断栈：为每个进程提供运行中断处理程序的栈，一页大小。

总之，历史上，进程和中断处理程序共享一个栈空间，当1页栈的选项激活之后，中断处理程序获得了自己的栈。

#### 12.11.2 在栈上工作

在任何函数，都要尽量节省内核栈的使用，让所有局部变量大小不要超过几百字节。栈溢出非常危险，所出的数据会直接覆盖紧邻堆栈末端的数据（例如`thread_info`结构就是紧邻进程堆栈末端）。

因此，推荐使用动态分配。

### 12.12 高端内存的映射

#### 12.12.1 永久映射

映射给定的page结构到内核地址空间，使用如下函数

```c
void *kmap(struct page *page);
```

函数在对于高端内存或者低端内存都能使用：

1. 如果page对应低端内存的一页，函数会单纯返回该物理页对应的虚拟地址
2. 如果page对应高端内存页，函数会建立一个永久映射，在返回对应的虚拟地址
3. 函数可以睡眠，只能在进程上下文中使用

当不再需要高端内存中的这一个页时，使用如下函数解除映射

```c
void kunmap(struct page *page);
```

#### 12.12.2 临时映射

>  当必须创建映射而上下文不能睡眠是，内核提供了临时映射（原子映射）

临时映射可以用在像中断上下文一样的不能睡眠的地方，使用如下函数建立心是映射：

```c
void *kmap_atomic(struct page *page, enum km_type type);
```

* 函数禁止了内核抢占（因为映射对每个处理器都是唯一的？？？）

### 12.13 每个CPU的分配

> SMP定义：一个操作系统的实例可以同时管理所有CPU内核，且应用并不绑定某一个内核。
>
> 支持SMP的操作系统使用每个CPU上的数据，对于给定的处理器其数据是唯一的，每个CPU的数据存放在一个数组中，数组的每一个元素对应一个存在的处理器

```c
unsigned long my_percpu[NR_CPUS];
```

访问cpu数据过程：

```c
int cpu;

cpu = get_cpu();  //获取当前CPU，并且禁止内核抢占
data = my_percpu[cpu];
....       // 使用data的过程
put_cpu();
```

代码中没有出现锁，因为数据对当前处理器是唯一的，没有其他处理器可以接触这个数据，没有多个处理器并发访问的问题，但是会有内核抢占的问题：

1. 如果代码被其他处理器抢占并重新调度，这是cpu变量data会变成无效，因为它对应了错误的处理器
2. 如果另一个进程抢占了代码，有可能在一个处理器上并发访问data数据的问题

因此，在获取当前cpu时，就已经禁止了内核抢占。

### 12.14 新的每个CPU接口

>  描述了一些为每个CPU分配内存的接口，详见**P223**

### 12.15 使用每个CPU数据的原因

使用每个CPU的好处有：

1. 减少数据锁定，每个处理器访问每个CPU数据，不用加锁
2. 使用每个CPU数据大大减少了缓存失效，失效发生在处理器试图使他们的缓存保持同步时，如果一个处理器操作数据时，该数据又存放在其他处理器缓存中，那么存放该数据的那个处理器必须刷新或者清理自己的缓存，频繁的缓存失效会造成**缓存抖动**

而使用每个CPU数据唯一的要求是需要**禁止内核抢占**



## 13 虚拟文件系统

> 虚拟文件系统（VFS）： 作为内核子系统，为用户空间程序提供了文件和文件系统相关的接口

### 13.1 通用文件模型

在处理文件时，内核空间和用户空间使用的主要对象是不同的。在用户空间，一个文件由一个**文件描述符**标识，在打开文件时由内核分配，只在一个**进程内部有效**；内核空间中处理文件的关键是**inode**，每个文件或目录有且只有一个对应的inode，其中包含元数据（如访问权限、修改日期等）和指向文件数据的指针，但**inode不包含文件名**

#### 13.1.1 inode

> 目录可看做一种特殊的文件

inode的成员分为两类：

* 描述文件状态的元数据
* 保存实际文件内容的数据段（或指向数据的指针）

举例说明，内核查找/usr/bin/emacs的过程：

* 查找起始于inode，对应于/目录，它对应一个inode，其数据段包含根目录下的各个目录项（这些目录项可能代表文件或目录），每个项由两个成员组成：
  1. 该目录项数据所在的inode编号
  2. 文件或目录的名称（文件名和inode之间的关联通过inode编号建立）
* 在/目录的inode数据段中查找名为usr的目录项，根据对应的inode编号定位/usr目录的inode；重复相同的步骤查找bin目录、emacs文件的inode
* 最后，emacs的inode数据段包含实际文件的内容

**注意**：VFS实际的文件查找过程和以上基本一致，但会有些细节差异，如实际的实现使用了缓存加速查找操作，另外VFS需要和提供实际信息的底层文件系统通信

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200618083457485.png" width="800">

#### 13.1.2 通用文件系统接口

> 在UNIX系统中，**万物皆文件**
>
> VFS使得用户可以直接使用统一的系统调用，无需考虑具体的文件系统和物理介质

大多数设备都通过VFS定义的文件接口访问：

* 字符和块设备
* 进程之间的管道
* 用于所有网络协议的套接字
* 用于交互式输入和输出的终端

> 内核在底层文件系统接口上建立了一个抽象层，使得Linux能够支持各种文件系统。
>
> 实际文件系统的代码在统一的接口和数据结构下隐藏各自具体的实现细节，它们通过编程提供VFS所期望的抽象接口和数据结构

### 13.2 VFS结构

>  VFS由**文件和文件系统**两部分组成

#### 13.2.1 结构概观

1. **文件的表示**

   对底层文件系统的操作使用**函数指针**来实现，他们保存在两个结构中：

   * inode操作：创建链接、文件重命名、在目录创建文件、删除文件
   * 文件操作：文件读写、设置文件位置指针、创建内存映射之类的操作

   每个inode包含一个指向底层文件系统超级块对象的指针，用于执行inode本身的一些操作

   文件和进程的联系：`task_struct`结构中的files数组，包含所有的打开文件（file结构），文件描述符作为数组的索引，同时file对象总包含一个指针指向用于**加速查找操作的目录项缓存dentry对象**

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200618084630339.png" width="800">

2. **文件系统和超级块信息**

   超级块包含了文件系统的关键信息（块长度、最大文件长度等），还有读、写、操作inode的函数指针

   内核建立了一个链表，包含了**所有活动的文件系统超级块实例**

   超级块结构中包含一个列表，包含相关文件系统**所有修改过的inode**（*脏inode*），用于回写到存储介质（磁盘等）

#### 13.2.2 Unix文件系统

> 四个基本要素：文件、目录项、索引节点和安装点（挂载点）

* 目录项：路径中的每一部分都被称为目录条目，统称为目录项
* 索引节点：Unix系统将文件的相关信息和文件本身这两个概念加以区分（如访问控制权限、大小、创建时间等），文件的相关信息（文件的元数据信息）被存储在一个单独的数据结构，称为索引节点（inode）
* 超级块：是一种包含文件系统控制信息的数据结构，这些信息称为文件系统数据元

![img](https://static001.geekbang.org/resource/image/32/47/328d942a38230a973f11bae67307be47.png)

### 13.3 VFS对象及数据结构

VFS的四个对象类型：

* 超级块对象：代表具体的文件系统
* 索引节点对象：代表具体文件
* 目录项对象：代表目录项，是路径的一个组成部分
* 文件对象：代表进程打开的文件

**注意**：VFS将目录作为一个文件来处理，不存在目录对象；目录项不同于目录

> 每个对象中都包含一个操作对象，其中描述了内核针对主要对象可以使用的方法

* `super_operations`对象：内核针对特定文件系统调用的方法，如`write_inode(), sync_fs()`
* `inode_operations`对象：内核针对特定文件调用的方法，如`create(), link()`
* `dentry_operations`对象：内核针对特定目录项所能调用的方法，如`d_compare(), d_delete()`
* `file_operations`对象：进程针对已打开文件所能调用的方法，如`read(), write()`

**注意**：操作对象作为结构体，其中包含操作父对象的函数指针，实际的文件系统可以继承VFS提供的通用函数。

#### 13.3.1 超级块

> 各种文件系统都必须实现超级块对象，该对象存储特定文件系统的信息，对应于存放在磁盘**特定扇区**中文件系统超级块或者文件系统控制块。（非基于磁盘的文件系统，会在使用现场创建超级块并保存在内存中）

1. **超级块结构super_block**

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619145306992.png">

主要成员：

* s_dirty：脏inode的链表，在同步内存数据和底层存储介质时，使用该链表更加高效
* s_files：包含一系列file结构，列出了该超级块表示的文件系统所有打开的文件，内核在卸载文件系统时会参考该链表
* s_dev和s_bdev指定了底层文件系统的数据所在的块设备，前者使用了内核内部编号，后者指向block_device结构
* s_root：用于检查文件系统是否装载，如果为NULL，该文件系统是一个伪文件系统，只在内核可见，否则，在用户空间可见

**注意**：超级块对象通过`alloc_super()`函数创建并初始化，在安装文件系统时，文件系统会调用这个函数从**磁盘**读取文件系统超级块，并将其中的数据**填充到内存中的超级块对象对应的结构体**中。

2. **超级块操作**

   超级块对象中s_op指针，指向超级块的操作函数表，由`super_operations()`表示，如下图：

   ![image-20200619145720447](D:\Tech\学习笔记\Linux学习笔记\image-20200619145720447.png)

![image-20200619145815123](D:\Tech\学习笔记\Linux学习笔记\image-20200619145815123.png)

该结构中的操作会控制**底层文件系统实现获取和返回inode的方法**，不会改变inode的内容

#### 13.3.2 inode对象

> 索引节点对象：包含内核在操作文件系统或者目录时需要的全部信息（对于Unix风格的系统，直接从磁盘的索引节点读入），索引节点对象必须在**内存**中创建

inode的结构如下：

![image-20200619135204831](D:\Tech\学习笔记\Linux学习笔记\image-20200619135204831.png)

![image-20200619135129369](D:\Tech\学习笔记\Linux学习笔记\image-20200619135129369.png)

上述的inode结构是在**内存**进行管理，其中包含实际介质中存储的inode没有的成员

inode结构主要成员：

* i_ino：唯一标识inode的编号
* i_count：访问该inode结构的进程数
* i_nlink：使用该inode的硬链接数目
* i_mode、i_uid和i_gid：文件访问权限和所有权
* i_rdev：设备号，用于找到struct block_device的一个实例
* 联合体
  * i_bdev：指向块设备结构block_device
  * i_pipe：实现管道的inode相关信息pipe_inode_info
  * i_cdev：指向字符设备结构cdev
* i_devices：这个成员作为链表元素，使得块设备或者字符设备维护一个inode的链表

1. **索引节点操作**

   inode结构包含两个指针：i_op和i_fop，前者指向inode_operation结构，提供了inode相关的操作，后者指向file_operations，提供了文件操作（inode和file结构都包含了指向file_operations结构的指针）

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619140433905.png">

2. **inode链表**

   每个inode都有一个ilist成员，将inode存储到一个链表中

   根据状态，inode分为三种：

   * inode存在于内存中，未关联到任何文件，不处于活动状态

   * inode存在于内存中，正在由一个或多个进程使用，两个计数器i_nlink和i_count都大于0，且文件内容和inode元数据和底层块设备相同

   * inode处于使用活动状态，内容已经改变，与存储介质上内容不同，inode是**脏的**

   在fs/inode.c文件中内核定义了两个全局变量表头，inode_unused用于有效但非活动的inode，inode_in_use用于正在使用但是未改变的inode。**脏的inode**保存在一个特定于超级块的链表中

#### 13.3.3 目录项缓存

> Linux使用**目录项缓存**（dentry缓存）来快速访问文件的查找结果

VFS在执行目录项操作时，会现场创建dentry实例，dentry实例没有对应的磁盘数据结构，并非保存在磁盘上，`dentry`结构体中没有是否被修改的标志（是否为脏、是否需要写会磁盘）

1. **dentry结构**

   dentry结构定义如下：

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619090453579.png" width="800">

   每个dentry代表路径中的一个特定部分，比如路径/bin/vi，其中/,bin,vi都是目录项，前两个是**目录**，最后一个是**普通文件**。在路径中，包含普通文件在内，每一项都是目录项对象。给定目录下的所有文件和子目录相关联的dentry实例，都归入到d_subdirs链表中，子目录节点的d_child作为链表元素

   dentry结构的**主要作用**：建立**文件名到inode之间映射关系的缓存**，涉及的结构有三个成员：

   * d_inode：指向inode实例的指针
   * d_name：指定文件的名称，qstr结构存储了**实际的文件名、文件长度和散列值**
   * d_iname：如果文件名由少量字符组成，保存在d_iname中（不是d_name），以加速访问

   内存中所有活动的dentry实例保存在一个散列表中，使用fs/dcache.c中的全局变量dentry_hashtable实现，d_hash实现溢出链，用于解决哈希冲突，每个元素指向具有相同键值的目录项组成的链表头指针，这个散列表称为**全局dentry散列表**(内核系统提供给文件系统**唯一**的散列函数)

   **注意**：dentry对象不是表示文件的主要对象，这一职责分配给inode

2. **dentry缓存的组织**

   每个由VFS发送到底层实现的请求，都会导致创建一个新的dentry对象，并保存请求结果

   dentry的**三种状态**：被使用、未使用和负状态

   * 被使用的目录项：对应一个有效的索引节点，`d_node`指向相应的索引节点，`d_count`代表使用者的数量；不能被丢弃
   * 未被使用的目录项：对应有效的索引节点，但是`d_count`为0，仍然指向一个有效对象，被保存在缓存中
   * 负状态的目录项：没有对应的有效索引节点，`d_node`为NULL，索引节点已被删除，或者路径不不再正确
   
   dentry对象**在内存中的组织**，涉及三个部分：
   
   * 一个全局散列表（dentry_hashtable）包含的所有dentry对象
   
   * **“被使用的”** 目录项链表：索引节点中`i_dentry`链接相关的目录项（一个索引节点可能有多个链接，对应多个目录项），因此用一个链表连接他们
   
   * **“最近被使用的”** 双向链表：包含未被使用和负状态的目录项对象（总是在头部插入新的目录项，需要回收内存时，会再尾部删除旧的目录项）
   
   **注意**：目录项释放后也可以保存在**slab缓存**中。
   
   * `dcache`一定意义上提供了对于索引节点的缓存（`icache`），和目录项相关的索引节点对象不会被释放（因为索引节点的使用计数>0），这样确保了索引节点留在内存中
   * 文件访问呈现空间和时间的局部性：时间局部性体现在程序在一段时间内可能会访问相同的文件；空间局部性体现在同一个目录下的文件很可能都被访问。

3. **dentry操作**

   dentry_operations结构中保存了一些对dentry对象执行的函数指针，在不同的文件系统中可以各自实现
   
   ![image-20200619114904496](D:\Tech\学习笔记\Linux学习笔记\image-20200619114904496.png)

#### 13.3.4 特定于进程的信息

>  三种结构体：`file_struct`, `fs_struct`, `namespace`

主要成员包括：

* fs指向的`fs_struct`结构，保存进程的文件系统相关数据
* files指向的`files_struct`结构，包含当前进程的各个文件描述符
* mnt_ns指向的`namespace`结构，包含命名空间相关信息

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619103702377.png" width="600">

1. **结构体 files_struct**

   结构定义如下：

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619104102379.png" width="800">
   
   主要成员：
   
   * `fd_array`数组指针：指向已打开的文件对象，`NR_OPEN_DEFAULT`默认64，如果进程打开的文件超过64，内核将分配一个新数组，并且将`fdt`指针指向它
   * 当访问的文件对象的数量小于64时，执行比较快，因为是对静态数组的操作；如果大于64，内核需要建立新数组，访问速度相对慢一些
   * 管理员可以增大`NR_OPEN_DEFAULT`选项优化性能
   * fdtable结构定义如下：

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619104457849.png" width="800">

2. **结构体 fs_struct**

   由进程描述符的`fs`域指向，包含**文件系统和进程相关的信息**，包含进程的当前工作目录（`pwd`）和根目录

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619111413066.png">

   其中dentry类型的成员指向目录名称，vfsmount类型成员表示已经装载的文件系统，成员如下：

   * umask成员：标准掩码，用于设置文件的权限
   * root和rootmnt指定相关进程的根目录和文件系统
   * pwd和pwdmnt指定**当前目录**和文件系统的vfsmount结构，在进程改变当前目录时，二者会动态改变（仅当进入一个新的挂载点的时候，pwdmnt才会变化）

3. **VFS命名空间**

   单一的系统可以提供多个容器，容器彼此相互独立，从VFS角度来看，需要**针对每个容器分别跟踪装载的文件系统**

   **VFS命名空间**是所有已经装载、构成某个容器目录树的文件系统集合。通过fork或clone建立的进程会继承父进程的命名空间，可以通过设置CLONE_NEWS标志，建立新的命名空间

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619113142088.png">

   进程描述符中的`nsproxy`成员管理命名空间的处理，其主要成员是`mmt_namespace`域，它使得**每个进程在系统中看到唯一的文件系统**（唯一的根目录和文件系统结构层次）

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619112545758.png">

   * count：使用计数器，指定了使用该命名空间的进程数
   * root：指向根目录挂载的vfsmount实例
   * list：一个双向链表的表头，保存了命名空间中所有文件系统的vfsmount实例，链表元素是vfsmount中的mnt_list成员

   对于容器来说，命名空间的操作（如mount和umount）并不作用于内核的全局vfsmount结构，只操作当前容器中进程的命名空间实例。同时，改变**会影响共享同一个命名空间实例的所有进程（容器）**

**注意**：

* 对于多数进程，它们的描述符都指向自己独有的`files_struct`和`fs_struct`，除非使用克隆标志`CLONE_FILES`或者`CLONE_FS`创建的进程会共享这两个结构体

   * `namespace`结构体使用方法和前两种结构完全不同，默认情况下，所有进程共享同样的命名空间（都从相同的挂载表中看到同一个文件系统层次结构，除非在`cloen()`操作时使用`CLONE_NEWS`标志，才会给进程一个命名空间结构体的拷贝）

#### 13.3.5 文件

> 文件对象是已打开的文件在内存中的表示

* 结构体`file`表示

* 由`open()`系统调用创建，`close()`撤销
* 多个进程可以打开同一个文件，所以同一个文件存在多个对应的file实例
* file对象仅仅在观点上代表已打开文件，它反过来指向dentry对象，而dentry对象反过来指向inode

类似于目录项对象，文件对象没有对应的磁盘数据，通过`file`结构体中`f_dentry`指针指向相关的目录项对象，而目录项对象指向相关的索引节点，索引节点会记录文件是否为脏

结构体`file_operation`定义如下：

<img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619111240263.png" width="1000">

文件相关的操作方法和**系统调用**很类似，具体的文件系统可以为每一种操作方法实现各自的代码，如果存在通用操作，则使用通用操作

#### 13.3.6 文件系统

所有文件系统都保存在一个**单链表**中，每个文件系统的名字存储为字符串。在文件系统注册到内核时，将逐个元素扫描该链表，直至到达尾部或者找到指定的文件系统

1. **结构体 file_system_type**

   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619142443132.png">

   主要成员：

   * name：文件系统的名称，字符串

   * fs_flags：使用的标志，标明只读装载等

   * next：指向下一个file_system_type结构

   * get_sb函数：从磁盘读取超级块，在文件系统安装时，在内存中组装超级块对象

   * kill_sb函数：在不在需要某个文件系统类型时执行清理工作

   * fs_supers：同一类型文件系统的所有超级块结构存储在一个链表中，fs_supers是这个链表的表头

   **注意**：相同类型的多个文件系统实例，都只有一个对应的`file_system_type`实例

2. **结构体 vfsmount**

   在文件系统实际被安装时，会有一个`vfsmount`结构体在安装点创建，每个装载的文件系统对应一个vfs_mount实例（代表一个**安装点**），`vfsmount`结构体中维护了各种链表，用于跟踪文件系统和所有安装点之间的关系

   系统使用了散列表mount_hashtable（定义在fs/namespace.c中），链表元素是vfs_mount类型，**vfs_mount实例的地址和相关的dentry实例的地址**用来计算散列和（哈希值）
   
   <img src="D:\Tech\学习笔记\Linux学习笔记\image-20200619144135833.png">

### 13.4 处理VFS对象

#### 13.4.1 文件系统操作

1. **注册文件系统**

   文件系统注册到内核时，是编译为模块，或者持久编译到内核。fs/super.c中的register_filesystem函数用来向内核注册文件系统，该函数扫描文件系统结构组成的单链表，直至到达链表尾部然后添加新的元素或者找到所需的文件系统

2. **装载和卸载**

   装载操作开始于**超级块**的读取，file_system_type中保存的read_super函数指针返回一个类型为super_block的对象，用来在内存中表示超级块

   **mount系统调用**

   入口点是sys_mount函数，在fs/namespace.c定义，代码流程如下图

   ![image-20200622083824843](D:\Tech\学习笔记\Linux学习笔记\image-20200622083824843.png)

   首先将装载选项（类型、设备和选项）从用户空间复制到内核空间，内核将控制权转给do_mount函数，该函数调用path_lookup找到装载点所在的dentry项

   do_mount充当一个多路分解器，将需要完成的工作委派给装载类型对应的各个函数，其中do_new_mount处理普通装载操作，它分为两部分：do_kern_mount和do_add_mount

   ![image-20200622085138977](D:\Tech\学习笔记\Linux学习笔记\image-20200622085138977.png)

   * do_kern_mount首先使用get_fs_type找到对应的file_system_type实例，并扫描已注册文件系统的链表，返回正确的项，然后调用特定于文件系统类型的get_sb函数读取相关的超级块，返回struct super_block的实例

   * do_add_mount首先处理一些必须的锁定操作，确保同一个文件系统不会重复装载到同一位置，然后主要工作是在graft_tree函数，新装载的文件系统通过attach_recursive_mnt函数添加到父文件系统的命名空间

     ![image-20200622090154215](D:\Tech\学习笔记\Linux学习笔记\image-20200622090154215.png)

     * nameidata用于将vfsmount实例和denrty实例聚集起来，该结构保存了装载点的dentry实例和装载之前该目录对应的vfsmount实例

     * mnt_set_mount确保新的vfsmount实例中的mnt_parent成员指向父文件系统的vfsmount实例，以及mnt_mountpoint成员指向装载点在父文件系统中的dentry实例，旧的dentry实例d_mounted值加一

       ![image-20200622090919581](D:\Tech\学习笔记\Linux学习笔记\image-20200622090919581.png)

     * 函数commit_tree将新的vfsmount实例添加到**全局散列表**以及**父文件系统vfsmount实例中的子文件系统链表**

       ![image-20200622091237024](D:\Tech\学习笔记\Linux学习笔记\image-20200622091237024.png)



   **umount**系统调用

   umount系统调用的入口点是fs/namespace.c中的sys_umount，如下图

![image-20200623100711987](D:\Tech\学习笔记\Linux学习笔记\image-20200623100711987.png)

   * 首先，__user_walk找到装载点的dentry和vfsmount实例，主要的工作委派给do_umount函数
   * 如果定义了特定于超级块的umount_begin函数，则调用它。例如，网络文件系统在卸载前，需要终止与远程文件系统的通信
   * 如果装在的文件系统不再需要（通过使用计数判断），或者指定了MNT_DETACH来强制卸载文件系统，则调用umount_tree和release_mounts函数，前者将d_mounted减一，后者使用保存在mnt_mountpoints和mnt_parent中的数据，将环境恢复到文件系统装在之前的状态。同时被卸载的文件系统对应的数据结构，会从内核链表中移除

#### 13.4.2 文件操作过程

1. **查找inode**

   主要操作是根据给定的文件名查找inode，nameidata结构用于向查找函数传递参数，并保存查找结果，该结构定义如下

   ![image-20200623102542286](D:\Tech\学习笔记\Linux学习笔记\image-20200623102542286.png)

   主要成员：

   * 查找完成后，dentry和mnt包含了找到的文件系统项的数据
   * last包含了查找的名称，包含字符串和散列值
   * flags保存了相关标志

   内核使用path_lookup函数查找路径和文件名，该函数需要一个nameidata类型的指针，用作临时结果的“暂存器”

   ![image-20200623191143202](D:\Tech\学习笔记\Linux学习笔记\image-20200623191143202.png)

   内核使用nameidata实例规定查找的起点，如果名称以/开头，使用当前根目录的dentry实例和vfsmount实例；否则，从当前进程的task_struct获得当前工作目录的数据

   主要处理在link_path_walk函数，它调用__link_path_walk函数，该函数代码流程图如下

   ![image-20200623191624065](D:\Tech\学习笔记\Linux学习笔记\image-20200623191624065.png)

   该函数由一个大的循环组成，逐分量处理文件或路径名（路径根据/被分解为多个分量），每个循环的主要逻辑为：

   * 将nameidata实例的mnt和dentry成员设置为根目录或工作目录对应的数据项
   * 对目录进行权限检查，判断进程是否允许进入该目录、
   * 路径名称是逐字母扫描，根据/将路径分为多个路径分量，每个循环处理一个路径分量；路径分量的每个字符传给partial_name_hash函数，用于计算散列和，将他保存到qstr实例中
   * 处理(**.**)，直接跳过
   * 处理(**..**)，委派给follow_dotdot函数，当前目录为进程的根目录时，没有效果，否则，分两种情况：
     * 如果当前目录不是一个**装载点**的根目录时，将当前dentry对象的d_parent成员作为新的目录
     * 如果是已装载文件系统的根目录，利用保存在mnt_mountpoint和mnt_parent中的信息定义下一个dentry和vfsmount对象。follow_mount和lookup_mnt用于取得所需的信息
   * 如果路径分量是一个普通文件，需要区分两种情况进行处理，数据位于dentry缓存 或者 需要文件系统底层实现进行查找，函数do_lookup负责区分两种情况，返回所需的dentry实例
   * 最后一步：判断该分量是否为符号链接（方法：只有勇于符号链接的inode，其inode_operations中才包含lookup函数，否则为NULL）

   最后一个分量对应的dentry作为函数link_path_walk的返回结果

2. **打开文件**

   标准库的open函数用于打开文件，该函数使用同名的open系统调用，调用了fs/open.c中的sys_open函数，代码流程图如下

   ![image-20200623193750366](D:\Tech\学习笔记\Linux学习笔记\image-20200623193750366.png)

   * force_o_largefile检查是否应该不考虑用户层传递的标志
   * 接下来的主要处理是在do_sys_open函数
     * 调用get_unused_fd_flags查找未使用的文件描述符
     * 根据open参数中的文件路径名称，查找对应的inode，主要是在do_file_open函数
       * open_namei函数调用path_lookup函数查找inode并执行几个额外检查
       * nameidata_to_filp初始化预读结构，将新创建的file实例放置到超级块的sfiles链表中，并调用底层文件系统随影file_operations中的open函数
   * fd_install函数将file实例放置到进程task_struct结构的files->fd数组中
   * 最后控制权转到用户进程，返回进程描述符

3. **读取和写入**

   文件打开之后，使用read和write系统调用进行读写，入口函数是sys_read和sys_write（都是在fs/read_write.c实现）

   * read

     函数需要三个参数：文件描述符、保存数据的缓冲区和指定读取字符数的长度参数，代码流程如下图

     * 根据文件描述符，fget_light函数（fs/file_table.c中）从task_struct结构中找到相关的file实例
     * file_pos_read找到文件当前的读写位置（返回file->f_pos的值）
     * vfs_read函数调用特定于文件的读取函数file->f_op->read，如果该函数没有实现，则调用一般的辅助函数do_sync_read
     * file_pos_write函数记录文件内部新的读写位置

     **注意**：读取数据涉及到复杂的缓冲区和缓存系统，详见[文件系统操作]()

     ![image-20200623194904111](D:\Tech\学习笔记\Linux学习笔记\image-20200623194904111.png)


### 13.5 VFS通用标准函数

对于文件读写的操作对于所有文件系统来说大同小异：如果数据所在的块是已知的，则首先查询页缓存；如果未保存在其中，则相对应的块设备发出请求。

为了防止每个文件系统在这部分的代码冗余，大多数文件系统将file_operations中的read和write分别指向do_sync_read和do_sync_write标准例程

#### 13.5.1 通用读取例程

do_sync_read例程同步地读取数据，保证在函数返回时，所需数据已经在内存中，实际的读取操作委托给一个异步例程。简化后函数如下

![image-20200713164051426](D:\Tech\学习笔记\Linux学习笔记\image-20200713164051426.png)

init_sync_kiocb函数初始化一个kiocb实例，用于控制异步输入/输出操作

实际工作委托给特定于文件系统的异步读取操作，保存在struct file_operations的aio_read成员，它指向generic_file_aio_read，该函数是异步的，无法保证返回时数据已经读取完毕

返回值-EIOCBQUEUED表示读请求正在排队，尚未处理，此时。wait_on_sync_kiocb将一直等待，直至数据进入内存。该函数根据创建的控制块，来检查请求的完成情况；在等待时，进程将进入睡眠状态，使得其他进程可以利用CPU

1. **异步读取**

   mm/filemap.c中的generic_file_aio_read异步读取数据，代码流程图如下

   ![image-20200713165518219](D:\Tech\学习笔记\Linux学习笔记\image-20200713165518219.png)

   在generic_segment_checks中确保读请求包含的参数有效之后，需要区分两种不同的读模式：

   * 如果设置了O_DIRECT标志，则直接读取数据，不使用页缓存，此时调用generic_file_direct_IO
   * 否则调用do_generic_file_read，然后调用do_generic_mapping_read函数，该函数将**对文件的读操作转换为对映射的读操作**

2. **对映射读取**

   do_generic_mapping_read代码流程图如下

   ![image-20200713174143283](D:\Tech\学习笔记\Linux学习笔记\image-20200713174143283.png)

   该函数使用映射机制，将文件中需要读取的部分映射到内存中，它由一个大的无限循环组成，持续向内存页读入数据，直至所有文件数据（不在任何缓存中的数据）都传输到内存中

   每个循环执行过程为：

   * find_get_page检查页是否在页缓存中
     * 如果没有，调用page_cache_sync_readahead发出同步预读请求，然后再次调用find_get_page保证数据已经进入缓存，如果此次调用还是没有进入缓存，则调用no_cached_page函数
     * 否则，如果设置了页标志PG_readahead，则发出异步预读请求，调用page_cache_async_readahead
   * 虽然页是在页缓存中，但数据不一定是最新的，需要调用Page_Uptodate检查，如果页不是最新的，必须使用mapping->a_ops->readpage再次读取，该函数指针通常指向mpage_readpage，调用之后，可以确保页中填充的数据是最新的
   * 接下来使用mark_page_accessed标记对该页的访问（在需要从物理内存换出数据时，这个标记会用于判断页的活动程度）
   * 最后，actor例程将适当的页映射到用户地址空间

   如果预读机制没有将所需的页读入内存，需要调用do_generic_mapping_read的no_cached_page函数，其代码流程图如下

   ![image-20200713185213374](D:\Tech\学习笔记\Linux学习笔记\image-20200713185213374.png)

   其中，page_cache_alloc_cold分配了一个缓存冷页，通过add_to_page_cache_lru将该页插入页缓存的LRU链表中，详见 [分配页](# 16.4.1 分配页)，映射提供的mapping -> a_ops -> readpage用于读取数据，通常该函数指向mpage_readpage，详见 [对整页的操作](# 16.4.4 对整页的操作)。最后，mark_page_accessed告诉统计系统该页已经访问过

#### 13.5.2 缺页异常处理

内存映射通常调用VFS提供的filemap_fault标准例程来读取未保存在缓存中的页，如下图是该函数的代码流程图

![image-20200713192813123](D:\Tech\学习笔记\Linux学习笔记\image-20200713192813123.png)



## 14 块I/O层



## 15 进程地址空间



## 16 页缓存和块缓存

内核为块设备提供了两种通用的缓存方案：

* 页缓存：针对以页为单位的所有操作，例如**内存映射技术**，负责了**块设备的大部分工作**
* 块缓存：以块为操作单位，存取的单位是设备的各个块，由于块长度取决于特定的文件系统，块缓存能处理不通长度的块

缓冲区曾经是块设备进行I/O操作的传统方法，目前只用于支持很小的读取操作，对于块传输的标准数据结构变为struct bio，这种方式可以合并统一请求中后续的块，加速处理，但是对于**单个块的操作**，缓冲区仍然是首选，例如经常按块读取元数据的系统。

很多场合下，页缓存和块缓存结合使用（一个缓存的页在写操作期间划分为不同的缓冲区，在更细的粒度识别出被修改的部分，在数据写回时，只需要回写被修改的部分，不需要整页传输）

### 16.1 页缓存的结构

#### 16.1.1 管理和查找缓存的页

Linux采用**基数树**的数据结构管理页缓存中的页，如下图

![image-20200703193455046](D:\Tech\学习笔记\Linux学习笔记\image-20200703193455046.png)



树的根有一个简单的数据结构表示，包含了树的高度（所包含节点的最大层次数目）和一个指针，指向组成树的第一个节点的数据结构

树的节点具备两种**搜索标记**，二者用于指定给定页当前是否为脏（即页的内容和后备存储器的数据不同）或该页是否正在向底层块设备回写。标记会一直向上设置到根节点，如果某个层次 *n+1* 的节点设置了某个标记，那么 *n* 层次的父节点也会设置该标记。好处是**内核可以判断在某个范围内是否有一页或多页设置了某个标记位**

#### 16.1.2 回写修改的数据

由于页缓存的存在，写操作不是直接对块设备进行，而是在内存中进行，修改的数据首先被收集起来，然后被传输到更低的内核层，在那里对写操作进一步优化，具体的优化过程详见 [块I/O层](##14 块I/O层)。这里从页缓存的视角来看，需要确定何时回写？

内核提供如下几种同步方案：

* 内核守护进程pdflush周期性地同步，他们扫描缓存中的页，将超出一定时间没有与底层块设备同步的页写回
* 如果缓存中修改的数据项数目在短期内内明显增加，内核会主动激活pdflush进程
* 提供了系统调用给用户调用来写回未同步的数据，常用的有sync调用

为管理可以按整页处理和缓存各种不同对象，内核使用了**“地址空间”**抽象，将内存中的页和特定的块设备关联起来，每个地址空间都有一个“宿主”，所谓其数据来源，一般用inode表示

一般修改文件或者按页缓存的对象时，只会修改页的一部分，为节约时间，在写操作期间，会将缓存中的每一页划分为较小的单位，称为**缓冲区**，回写过程中以缓冲区为单位来操作

### 16.2 块缓存的结构

Linux早期版本只包含块缓存，用于加速文件操作和系统性能，底层块设备的块缓存存在内存的缓冲区中，可以加速读写（实现部分包含在fs/buffers.c中）

与内存页相比，块比较小而且长度可变（依赖于使用的块设备或者文件系统）

文件系统在处理元数据时，一般会使用块缓存；而裸数据的传输则按页进行

缓冲区的实现基于页缓存，Linux 2.6之前，缓冲区使用缓冲区头buffer head结构实现，在2.6之后，不再使用缓冲区头结构，而是使用bio结构

### 16.3 地址空间

地址空间建立了缓存数据与后备存储器之间的关联，实现了两个单元之间的转换机制

* 内存中的页关联到每个地址空间，这些页表示缓存的内容

* 后备存储器指定了填充地址空间中页的数据的来源，它是虚拟内存中区域到后备存储器（块设备）上对应位置的映射

#### 16.3.1 数据结构

地址空间的基础是address_struct结构，定义如下：

![image-20200707091245724](D:\Tech\学习笔记\Linux学习笔记\image-20200707091245724.png)
![image-20200707091319282](D:\Tech\学习笔记\Linux学习笔记\image-20200707091319282.png)

主要成员：

* 与地址空间管理的区域之间的关联，通过两个成员建立：host指向inode实例，指定了后备存储器；一个基数树的根page_tree列出了地址空间中所有的物理内存页
* i_mmap是一棵树的根节点，包含了与该inode相关的所有普通内存映射，该树的作用在于，支持查找给定区间至少一页的所有内存区域

![image-20200707091505999](D:\Tech\学习笔记\Linux学习笔记\image-20200707091505999.png)

* backing_dev_info是一个指针，指向另一个结构，包含了与地址空间相关的后备存储器的有关信息（后备存储器是指与地址空间相关的外部设备，用作地址空间中信息的来源，通常为**块设备**）
* aps指针指向address_space_operation结构，其中包含了一组函数指针，指向用于处理地址空间的特定操作

地址空间与内核其他部分的关联如下图

![image-20200707092746157](D:\Tech\学习笔记\Linux学习笔记\image-20200707092746157.png)

#### 16.3.2 页树



### 16.4 页缓存的实现

#### 16.4.1 分配页

page_cache_alloc用于为一个即将加入页缓存的新页分配数据结构，加上后缀_cold的函数是获取一个冷页

![image-20200707093319597](D:\Tech\学习笔记\Linux学习笔记\image-20200707093319597.png)

* 首先page_cache_alloc将工作委托给alloc_pages，它从**伙伴系统**获得一个页帧

* 接下来将新页添加到页缓存，这是在add_to_page_cache函数中实现，如下所示，radix_tree_insert将与页相关的page实例插入到地址空间的基数树，在页缓存中的索引和指向所属地址空间的指针保存在page的成员index和mapping中

  ![image-20200707093832701](D:\Tech\学习笔记\Linux学习笔记\image-20200707093832701.png)

内核还提供了另一个可选的参数add_to_page_cache_lru，他首先调用add_to_page_cache向地址空间的页缓存添加一页，然后使用lru_cache_add函数将该页加入到系统的LRU缓存

#### 16.4.2 查找页

使用基数树判断给定页是否已经缓存，使用find_get_page实现该功能

![image-20200707094443166](D:\Tech\学习笔记\Linux学习笔记\image-20200707094443166.png)

其中radix_tree_lookup用于查找位于给定偏移量的页，在找到页之后，page_cache_get将页的引用计数加1

#### 16.4.3 在页上等待

内核经常要在页上等待，直至其状态改变为预期值。例如，数据同步时需要确保对于某页的回写已经结束，此时内存页的内容和底层块设备是相同的。处于回写过程中的页会设置PG_writeback标志位

内核提供了wait_on_page_writeback函数，用于等待页的该标志位清除

![image-20200707095202477](D:\Tech\学习笔记\Linux学习笔记\image-20200707095202477.png)
![image-20200707095225496](D:\Tech\学习笔记\Linux学习笔记\image-20200707095225496.png)

wait_on_cahe_bit安装一个等待队列，进程可以在上面睡眠，直至PG_writeback标志位清除

另外也有等待页解锁的需求，使用函数wait_on_page_locked实现

#### 16.4.4 对整页的操作

内核在块设备和内存之间传输数据时，相关的算法和数据结构都是以页为基本单位，而逐个缓冲区/块的传输会对性能产生负面影响。因此，在再重新设计块层的过程中，内核版本2.5引入**BIO**，来替代缓冲区，用于处理与块设备的数据传输。内核添加了4个函数，来支持读写一页或多页

![image-20200713152306980](D:\Tech\学习笔记\Linux学习笔记\image-20200713152306980.png)

其中，writeback_control用于精确控制回写操作的选项

这4个函数共同之处：都是构建一个BIO实例，用于对块层进行传输

以mpage_readpages为例，该函数需要nr_pages个page实例，以链表的形式通过参数pages传递进去，mapping是相关的地址空间，get_block用于查找匹配的块地址

该函数首先遍历所有的page实例，在循环的每一遍，首先将该页添加到地址空间相关的页缓存中，然后创建一个bio请求，从块层读取所需的数据

![image-20200713153304300](D:\Tech\学习笔记\Linux学习笔记\image-20200713153304300.png)
![image-20200713153412875](D:\Tech\学习笔记\Linux学习笔记\image-20200713153412875.png)
![image-20200713153434116](D:\Tech\学习笔记\Linux学习笔记\image-20200713153434116.png)

在do_mpage_readpage建立bio请求时，会包含此前各页的BIO数据，以构造一个合并的请求（将几页的读取合并到一个请求，而不是每页一个请求）

如果在循环结束时，do_mpage_readpage留下一个未处理的BIO请求，则提交该请求

![image-20200713153535101](D:\Tech\学习笔记\Linux学习笔记\image-20200713153535101.png)

#### 16.4.5 页缓存预读

页缓存预读不是由页缓存独立完成的，还需要VFS和内存管理层的支持

预读在内核的几处都有涉及

* do_generic_mapping_read：一个内核通用的读取例程
* filemap_fault函数：缺页异常处理程序，负责为内存映射读取缺页

下面以do_generic_mapping_read为例，来考察预读的具体过程

![image-20200713154924924](D:\Tech\学习笔记\Linux学习笔记\image-20200713154924924.png)

假定进程已经打开了一个文件，准备读取第一页，该页尚未读入页缓存，此时不会只读入一页，而是顺序读取多页，内核调用page_cache_sync_readahead读取一行中的8页（数字8是具体说明），第一页对于do_generic_mapping_read来说是立即可用的，而在实际需要之前就被读入页缓存的页，处于**预读窗口**中

之后进程继续读取接下来的页，在访问第6页（6是举例说明）时，内核检测到在该页设置了PG_Readahead标志。此时触发了一个异步操作，会在后台读取若干页（由于还有两页可用，所以不需要同步读取，但在后台进行的I/O操作需要确保进一步读取文件时，相关页已经读入页缓存）。page_cache_async_read函数负责发出异步读请求，它又会将窗口中的一页标记为PG_Readahead，在进程遇到该页时，又会触发异步读取，以此类推

最重要的问题在于**预测预读窗口的最优长度**。因此，内核会记录每个文件上一次的设置，使用file_ra_state关联到每个file实例

![image-20200713160800427](D:\Tech\学习笔记\Linux学习笔记\image-20200713160800427.png)

主要成员：

* start：表示页缓存开始预读的位置
* size：表示预读窗口的长度
* async_size：表示剩余预读页的最小值，如果预读窗口中的页数等于这个值，则会触发异步预读
* ra_pages：表示预读窗口的最大长度，内核读入的页数可以小于这个值，但是不能超过
* prev_pos：表示前一次读取时，最后访问的位置

预读机制的实现涉及如下几个函数

![image-20200713160114414](D:\Tech\学习笔记\Linux学习笔记\image-20200713160114414.png)

其中，ondemand_readahead例程负责实现预读策略（即判断预读多少当前不需要的页），在确定预读窗口的长度之后，调用ra_submit，将技术性问题委托给__do_page_cache_readhead函数，其中页是在页缓存中分配的，而后由块层填充


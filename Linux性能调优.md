[toc]



# Linux性能调优

## CPU性能篇

### 03 CPU上下文切换（上）

> CPU上下文：包括CPU寄存器和程序计数器
>
> CPU寄存器：是 CPU 内置的容量小、但速度极快的内存
>
> 程序计数器：是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置

![img](https://static001.geekbang.org/resource/image/98/5f/98ac9df2593a193d6a7f1767cd68eb5f.png)

* CPU上下文切换：是先把前一个任务的 CPU 上下文（也就是**CPU 寄存器和程序计数器**）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务
* 这些这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来

根据任务的不同，CPU的上下文切换分为进程上下文切换、线程上下文切换和终端航下文切换

#### 进程上下文切换

Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。

* 内核空间（Ring 0）具有最高权限，可以直接访问所有资源；
* 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。

![img](https://static001.geekbang.org/resource/image/4d/a7/4d3f622f272c49132ecb9760310ce1a7.png)

从进程用户态到内核态的转变，需要通过**系统调用**来完成，系统调用的过程中会发生**两次CPU上下文切换**。CPU里原来用户态指令的执行位置需要先保存起来，然后更新为内核态执行的指令位置，最后跳转到内核态运行内核任务。在系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。

**注意**：

1. 系统调用的过程中，不会涉及到虚拟内存等进程态的资源，不会切换进程，系统调用过程和进程上下文切换不一样，整个过程都是同一个进程

2. 系统调用称为特权模式切换，不是上下文切换

进程上下文切换和系统调用的区别：

进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的**虚拟内存、栈**等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈

![img](https://static001.geekbang.org/resource/image/39/6b/395666667d77e718da63261be478a96b.png)

保存上下文和恢复上下文的过程需要内核在CPU上运行才能完成，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。

在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间，从而导致系统平均负载升高。

Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在**多处理器系统**上，缓存是被多个处理器**共享**的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。

Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。

进程被CPU重新调度的时机：

1. 进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行
2. 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行
3. 进程在系统**资源**不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行
4. 进程通过睡眠函数  sleep 这样的方法将自己主动挂起时，自然也会重新调度
5. 有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行
6. 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序

#### 线程上下文切换

线程和进程的区别：**线程是调度的基本单位，而进程则是资源拥有的基本单位**。

所谓内核中的任务调度，实际上的调度对象是**线程**；而进程只是给线程提供了虚拟内存、全局变量等资源。

* 当进程只有一个线程时，可以认为进程就等于线程
* 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的
* 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的

因此，线程的上下文切换分为两种情况：

1. 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样
2. 前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的**私有数据、寄存器**等不共享的数据

**注意**：同进程的线程切换要比进程间的切换消耗更少的资源，更加轻量级

#### 中断上下文切换

为了响应硬件事件，**中断处理会打断进程的正常调度和执行**，转而调用中断处理程序，响应设备事件。

中断上下文切换不会涉及到进程的用户态，中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等

对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以**中断上下文切换并不会与进程上下文切换同时发生**

大部分中断处理程序都短小精悍，以便尽可能快的执行结束。

中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能



## IO性能篇

### 23 Linux文件系统如何工作

#### 索引节点和目录项

* 索引节点，简称inode，和文件一一对应，存储在磁盘中，记录文件的元数据
* 目录项，dentry，记录文件的名字、索引节点以及其他目录项的关联关系



举例说明，为文件创建的硬链接，会对应不同的目录项，他们都连接到同一个文件，索引节点相同



磁盘的最小单位是**扇区**，文件系统将连续的扇区组成逻辑块，以逻辑块为最小单位，来读写磁盘数据。常见的逻辑块4KB，由连续的8个扇区组成。

**示意图**

![img](https://static001.geekbang.org/resource/image/32/47/328d942a38230a973f11bae67307be47.png)



磁盘在执行文件系统格式化时，分为三个区域：超级块、索引节点和数据块区

* 超级块：整个文件系统的状态
* 索引节点区：存储索引节点
* 数据块区：存储文件数据



#### 虚拟文件系统

**示意图**

![img](https://static001.geekbang.org/resource/image/72/12/728b7b39252a1e23a7a223cdf4aa1612.png)

文件系统分类：

* 基于磁盘的文件系统：常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统
* 基于内存的文件系统：常说的虚拟文件系统，不需要磁盘空间，但是占用内存。比如，/proc和/sys
* 网络文件系统：用来访问其他计算机的文件系统，比如NFS、SMB、iSCSI 等

**注意**：这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为**挂载点**），然后才能访问其中的文件。



#### 文件系统IO

根据是否利用标准库缓存，分为缓冲IO和非缓冲IO

* 缓存IO：利用标准库缓，加速文件访问，标准库内部利用系统调用访问文件
* 非缓存IO：直接通过系统调用访问文件，不再经过标准库缓存

**注意**：这里的“缓冲”，是指标准库内部实现的缓存，最终还是需要通过系统调用，而系统调用还会通过**页缓存**，来减少磁盘的IO操作



根据是否利用操作系统的页缓存，分为直接IO和非直接IO

* 直接IO：跳过操作系统的页缓存，直接和**文件系统**交互来访问文件
* 非直接IO：先通过页缓存，再通过内核或者额外的系统调用，真正和磁盘交互（`O_DIRECT`标志）



根据应用程序是否阻塞自身，分为阻塞IO和非阻塞IO



根据是否等待相应结果，分为同步IO和异步IO

* 同步IO：应用程序执行IO操作之后，要等到整个IO完成后，才能获得IO响应
* 异步IO：应用程序不用等待IO完成，会继续执行，等到IO执行完成，会以事件的方式通知应用程序

设置`O_SYNC`或者`O_DSYNC`，代表同步IO。如果是`O_DSYNC`，要等到文件数据写入磁盘之后，才能返回，如果是`O_SYNC`，是在`O_DSYNC`的基础上，要求文件**元数据**写入磁盘，才返回



设置`O_ASYNC`，代表异步IO，系统会再通过`SIGIO`或者`SIGPOLL`通知进程

#### 性能观测

##### 容量

`df`命令查看磁盘空间

```bash
$ df -h /dev/sda1 
Filesystem      Size  Used Avail Use% Mounted on 
/dev/sda1        29G  3.1G   26G  11% / 

# 查看索引节点所占的空间
$ df -i /dev/sda1 
Filesystem      Inodes  IUsed   IFree IUse% Mounted on 
/dev/sda1      3870720 157460 3713260    5% / 
```

当索引节点空间不足，但是磁盘空间充足时，很可能是过多小文件导致的。**解决方法**一般是删除这些小文件，或者移动到索引节点充足的其他磁盘区

##### 缓存

可以使用free或者vmstat，观察页缓存的大小

也可以查看/proc/meminfo

```bash
$ cat /proc/meminfo | grep -E "SReclaimable|Cached" 
Cached:           748316 kB 
SwapCached:            0 kB 
SReclaimable:     179508 kB 
```

内核使用slab机制，管理目录项和索引节点的缓存，/proc/meminfo给出了整体的slab大小，/proc/slabinfo可以查看每一种slab的缓存

```bash

$ cat /proc/slabinfo | grep -E '^#|dentry|inode' 
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail> 
xfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 
... 
ext4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 
sock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 
shmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 
proc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 
inode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 
dentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 
```

其中，dentry代表目录项缓存，inode_cache代表VFS索引节点缓存，其他的就是各种文件系统的索引节点缓存



实际性能分析中，更常使用slabtop命令，来找出占用内存最多的缓存类型

示例如下：可以看到，目录项和索引节点占用了最多的 Slab 缓存，总共大约23M

```bash

# 按下c按照缓存大小排序，按下a按照活跃对象数排序 
$ slabtop 
Active / Total Objects (% used)    : 277970 / 358914 (77.4%) 
Active / Total Slabs (% used)      : 12414 / 12414 (100.0%) 
Active / Total Caches (% used)     : 83 / 135 (61.5%) 
Active / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) 
Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K 

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME 
69804  23094   0%    0.19K   3324       21     13296K dentry 
16380  15854   0%    0.59K   1260       13     10080K inode_cache 
58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache 
   485    413   0%    5.69K     97        5      3104K task_struct 
  1472   1397   0%    2.00K     92       16      2944K kmalloc-2048 
```




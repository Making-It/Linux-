[toc]



# Linux性能调优

## CPU性能篇

### 3 CPU上下文切换（上）

> CPU上下文：包括CPU寄存器和程序计数器
>
> CPU寄存器：是 CPU 内置的容量小、但速度极快的内存
>
> 程序计数器：是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置

![img](https://static001.geekbang.org/resource/image/98/5f/98ac9df2593a193d6a7f1767cd68eb5f.png)

* CPU上下文切换：是先把前一个任务的 CPU 上下文（也就是**CPU 寄存器和程序计数器**）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务
* 这些这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来

根据任务的不同，CPU的上下文切换分为进程上下文切换、线程上下文切换和中断下文切换

#### 3.1 进程上下文切换

Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。

* 内核空间（Ring 0）具有最高权限，可以直接访问所有资源；
* 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。

![img](https://static001.geekbang.org/resource/image/4d/a7/4d3f622f272c49132ecb9760310ce1a7.png)

从进程用户态到内核态的转变，需要通过**系统调用**来完成，系统调用的过程中会发生**两次CPU上下文切换**。CPU里原来用户态指令的执行位置需要先保存起来，然后更新为内核态执行的指令位置，最后跳转到内核态运行内核任务。在系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。

**注意**：

1. 系统调用的过程中，不会涉及到虚拟内存等进程态的资源，不会切换进程，系统调用过程和进程上下文切换不一样，整个过程都是同一个进程

2. 系统调用称为特权模式切换，不是上下文切换

进程上下文切换和系统调用的区别：

进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的**虚拟内存、栈**等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈

![img](https://static001.geekbang.org/resource/image/39/6b/395666667d77e718da63261be478a96b.png)

**保存上下文和恢复上下文的过程需要内核在CPU上运行才能完成**（上下文切换过程是CPU密集型），每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。

在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在**寄存器、内核栈以及虚拟内存**等资源的保存和恢复上，进而大大缩短了真正运行进程的时间，从而导致系统平均负载升高。

Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在**多处理器系统**上，缓存是被多个处理器**共享**的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。

Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是**优先级最高和等待 CPU 时间最长**的进程来运行。

进程被CPU重新调度的时机：

1. 进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行
2. 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行
3. 进程在系统**资源**不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行
4. 进程通过睡眠函数  sleep 这样的方法将自己主动挂起时，自然也会重新调度
5. 有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行
6. 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序

#### 3.2 线程上下文切换

线程和进程的区别：**线程是调度的基本单位，而进程则是资源拥有的基本单位**。

所谓内核中的任务调度，实际上的调度对象是**线程**；而进程只是给线程提供了虚拟内存、全局变量等资源。

* 当进程只有一个线程时，可以认为进程就等于线程
* 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的
* 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的

因此，线程的上下文切换分为两种情况：

1. 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样
2. 前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的**私有数据、寄存器**等不共享的数据

**注意**：同进程的线程切换要比进程间的切换消耗更少的资源，更加轻量级

#### 3.3 中断上下文切换

为了响应硬件事件，**中断处理会打断进程的正常调度和执行**，转而调用中断处理程序，响应设备事件。

中断上下文切换不会涉及到进程的用户态，它其实只包括内核态中断服务程序执行所必需的状态，包括**CPU 寄存器、内核堆栈、硬件中断参数**等

对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以**中断上下文切换并不会与进程上下文切换同时发生**

大部分中断处理程序都短小精悍，以便尽可能快的执行结束。

中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能



### 4 CPU上下文切换（下）

#### 4.1 查看上下文切换

1. 查看系统总体情况

```bash
# 每隔5s输出一组数据
[root@VM_194_74_centos ~]# vmstat 5 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 118120 1188128 13137072    0    0     0    10    0    0  3  1 97  0  0
 1  0      0 117040 1188128 13137080    0    0     0    22 1071 1311  1  0 99  0  0
 0  0      0 116824 1188128 13137092    0    0     0    13 1181 1421  1  0 99  0  0
 0  0      0 117328 1188128 13137100    0    0     0    12 1165 1374  1  0 99  0  0
 1  0      0 117168 1188128 13137112    0    0     0    22 1148 1391  1  0 99  0  0
```

参数：

* `cs`：context switch，每秒上下文切换的次数
* `in`：interrupt ，每秒中断的次数
* `r`：就绪队列的长度（正在运行和等待CPU的进程数）
* `b`：blocked，处于不可中断睡眠状态的进程数

2. 查看进程的详细信息

命令：pidstat

```bash
[root@VM_194_74_centos ~]# pidstat -w 5
Linux 3.10.107-1-tlinux2_kvm_guest-0049 (VM_194_74_centos)      05/07/20        _x86_64_        (8 CPU)

08:20:54      UID       PID   cswch/s nvcswch/s  Command
08:20:59        0         1      0.80      0.00  systemd
08:20:59        0         7      0.80      0.00  migration/0
08:20:59        0         9     90.40      0.00  rcu_sched
08:20:59        0        10      0.20      0.00  watchdog/0
08:20:59        0        11      0.20      0.00  watchdog/1
08:20:59        0        12      2.00      0.00  migration/1
08:20:59        0        16      0.20      0.00  watchdog/2
08:20:59        0        17      1.00      0.00  migration/2
08:20:59        0        18      0.20      0.00  ksoftirqd/2
...
```

参数：

* cswch：每秒自愿上下文切换的次数（voluntary context switch）
* nvcswch：每秒非自愿上下文切换的次数(non voluntary context switch)

> 自愿上下文切换：进程无法获取所需资源导致的上下文切换，比如I/O，内存等**系统资源不足**时发生的上下文切换
>
> 非自愿上下文切换：进程因**时间片**已到等原因，被系统强制**调度**发生的上下文切换，比如多个进程**竞争**CPU是发生的上下文切换

#### 4.2 案例分析

`sysbench`模拟多线程调度切换

##### 4.2.1 准备

> 一台Linux机器，打开三个终端

##### 4.2.2 正式实战

1. 第一个终端：运行`sysbench`

```
# 以10个线程运行5分钟的基准测试，模拟多线程切换的问题
$ sysbench --threads=10 --max-time=300 threads run
```

2. 第二个终端：运行`vmstat`

```bash
# 每隔1秒输出1组数据（需要Ctrl+C才结束）
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 6  0      0 6487428 118240 1292772    0    0     0     0 9019 1398830 16 84  0  0  0
 8  0      0 6487428 118240 1292772    0    0     0     0 10191 1392312 16 84  0  0  0
```

指标观察：

* cs列：上升到39万

* r列：就绪队列长度上升到8
* in列：终端次数上升到1万
* us（user）和sy（system）列：使用率加起来100%，sy为84%，主要被内核占用

3. 查看进程情况

```bash

# 每隔1秒输出1组数据（需要 Ctrl+C 才结束）
# -w参数表示输出进程切换指标，而-u参数则表示输出CPU使用指标
$ pidstat -w -u 1
08:06:33      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
08:06:34        0     10488   30.00  100.00    0.00    0.00  100.00     0  sysbench
08:06:34        0     26326    0.00    1.00    0.00    0.00    1.00     0  kworker/u4:2

08:06:33      UID       PID   cswch/s nvcswch/s  Command
08:06:34        0         8     11.00      0.00  rcu_sched
08:06:34        0        16      1.00      0.00  ksoftirqd/1
08:06:34        0       471      1.00      0.00  hv_balloon
08:06:34        0      1230      1.00      0.00  iscsid
08:06:34        0      4089      1.00      0.00  kworker/1:5
08:06:34        0      4333      1.00      0.00  kworker/0:3
08:06:34        0     10499      1.00    224.00  pidstat
08:06:34        0     26326    236.00      0.00  kworker/u4:2
08:06:34     1000     26784    223.00      0.00  sshd
```

**分析**：CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat  ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd

### 5 系统出现大量不可中断进程和僵尸进程怎么办？

#### 5.1 进程状态

* **R**：表示正在就绪队列中的进程，正在运行或者正在等待运行
* **D**：Disk Sleep，不可中断状态睡眠（Uninterruptible Sleep），一般是进程和硬件交互，并且交互过程不允许其他进程或中断打断
* **Z** ：Zombie 的缩写，它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）
* **S** ：Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态
* **I**： Idle 的缩写，也就是空闲状态，用在**不可中断睡眠的内核线程**上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会
* **T**：Stopped或者Traced，表示进程处于暂停或者跟踪状态（SIGSTOP信号会让进程变为暂停状态，再发送SIGCONT信号，进程又会恢复运行）
* **X**：Dead，表示进程已经消亡，top或者ps看不到

>  不可中断状态，是为了保证进程数据与硬件状态一致，正常情况下，不可中断状态在很短时间内就会结束。短时的不可中断状态进程，我们一般可以忽略。
>
> 但如果系统或硬件发生了故障，进程可能会在不可中断状态保持很久，甚至导致系统中出现大量不可中断进程。需要注意下，系统是不是出现了 I/O 等性能问题。

**注意**：ps查看进程状态时，会有Ss+，D+等情况，其中s表示进程是会话的领导进程，+表示前台进程组



#### 5.2 案例分析

##### 5.2.1 指标分析

1. 运行案例的docker

```bash
docker run --privileged --name=app -itd feisky/app:iowait
```

2. top查看指标

```bash
# 按下数字 1 切换到所有 CPU 的使用情况，观察一会儿按 Ctrl+C 结束
$ top
top - 05:56:23 up 17 days, 16:45,  2 users,  load average: 2.00, 1.68, 1.39
Tasks: 247 total,   1 running,  79 sleeping,   0 stopped, 115 zombie
%Cpu0  :  0.0 us,  0.7 sy,  0.0 ni, 38.9 id, 60.5 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  0.7 sy,  0.0 ni,  4.7 id, 94.6 wa,  0.0 hi,  0.0 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:38.59 systemd
...
```

3. 分析

* 第一行的平均负载（ Load Average），过去 1 分钟、5 分钟和 15 分钟内的平均负载在依次减小，说明平均负载正在升高；而 1 分钟内的平均负载已经达到系统的 CPU 个数，说明系统很可能已经有了性能瓶颈。
* 第二行的 Tasks，有 1 个正在运行的进程，但僵尸进程比较多，而且还在不停增加，说明有子进程在退出时没被清理。
*  CPU 的使用率情况，用户 CPU 和系统 CPU 都不高，但 iowait 分别是 60.5% 和 94.6%，好像有点儿不正常。
* 最后再看每个进程的情况， CPU 使用率最高的进程只有 0.3%，看起来并不高；但有两个进程处于 D 状态，它们可能在等待 I/O，但光凭这里并不能确定是它们导致了 iowait 升高。

4. 结论

* 第一点，iowait 太高了，导致系统的平均负载升高，甚至达到了系统 CPU 的个数
* 第二点，僵尸进程在不断增多，说明有程序没能正确清理子进程的资源。

##### 5.2.2 iowait分析

1. dstat查看系统I/O情况

```bash
# 间隔1秒输出10组数据
$ dstat 1 10
You did not select any stats, using -cdngy by default.
--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai stl| read  writ| recv  send|  in   out | int   csw
  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885
  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135
  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177
  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144
  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147
  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131
  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168
  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134
```

可以看到，每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的

2. pidstat分析D状态的进程

```bash
# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据
$ pidstat -d -p 4344 1 3
06:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:38:51        0      4344      0.00      0.00      0.00       0  app
06:38:52        0      4344      0.00      0.00      0.00       0  app
06:38:53        0      4344      0.00      0.00      0.00       0  app
```

* kB_rd 表示每秒读的 KB 数
*  kB_wr 表示每秒写的 KB 数
* iodelay 表示 I/O 的延迟（单位是时钟周期）。
* 它们都是 0，那就表示此时没有任何的读写，说明问题不是 4344 进程导致的。

3. pidstat查看所有进程情况

```bash
# 间隔 1 秒输出多组数据 (这里是 20 组)
$ pidstat -d 1 20
...
06:48:46      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:47        0      4615      0.00      0.00      0.00       1  kworker/u4:1
06:48:47        0      6080  32768.00      0.00      0.00     170  app
06:48:47        0      6081  32768.00      0.00      0.00     184  app

06:48:47      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:48        0      6080      0.00      0.00      0.00     110  app

06:48:48      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:49        0      6081      0.00      0.00      0.00     191  app

06:48:49      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command

06:48:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:51        0      6082  32768.00      0.00      0.00       0  app
06:48:51        0      6083  32768.00      0.00      0.00       0  app

06:48:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:52        0      6082  32768.00      0.00      0.00     184  app
06:48:52        0      6083  32768.00      0.00      0.00     175  app

06:48:52      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:53        0      6083      0.00      0.00      0.00     105  app
...
```

观察一会儿可以发现，的确是 app 进程在进行磁盘读，并且每秒读的数据有 32 MB，看来就是 app 的问题。不过，app 进程到底在执行啥 I/O 操作呢？**进程想要访问磁盘，就必须使用系统调用，所以接下来，重点就是找出  app 进程的系统调用**

4. strace跟踪进程

```bash
$ strace -p 6082
strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted
```

* 检查一下进程的状态，已经变成僵尸进程

```bash
$ ps aux | grep 6082
root      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] <defunct>
```

5. 动态追踪

```bash
$ perf record -g
$ perf report
```

如下图，swapper是内核的调度进程，可忽略

可以发现， app 的确在通过系统调用 **sys_read()** 读取数据。并且从 new_sync_read 和 blkdev_direct_IO  能看出，进程正在对磁盘进行**直接读**，也就是**绕过了系统缓存**，每个读请求都会从磁盘直接读，这就可以解释我们观察到的 iowait 升高了

![img](https://static001.geekbang.org/resource/image/21/a1/21e79416e946ed049317a4b4c5a576a1.png)

6. 直接查看docker内代码

打开app.py文件，可以看到使用了 O_DIRECT 选项打开磁盘

```python
open(disk, O_RDONLY|O_DIRECT|O_LARGEFILE, 0755)
```

> 直接读写磁盘，对 I/O 敏感型应用（比如数据库系统）是很友好的，因为你可以在应用中，直接控制磁盘的读写。但在大部分情况下，我们最好还是通过系统缓存来优化磁盘 I/O

7. 修复代码

修复后的文件名app-fix1.py，运行docker如下

```
# 首先删除原来的应用
$ docker rm -f app
# 运行新的应用
$ docker run --privileged --name=app -itd feisky/app:iowait-fix1
```

top检查

```bash
$ top
top - 14:59:32 up 19 min,  1 user,  load average: 0.15, 0.07, 0.05
Tasks: 137 total,   1 running,  72 sleeping,   0 stopped,  12 zombie
%Cpu0  :  0.0 us,  1.7 sy,  0.0 ni, 98.0 id,  0.3 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  1.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3084 root      20   0       0      0      0 Z   1.3  0.0   0:00.04 app
 3085 root      20   0       0      0      0 Z   1.3  0.0   0:00.04 app
    1 root      20   0  159848   9120   6724 S   0.0  0.1   0:09.03 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
    3 root      20   0       0      0      0 I   0.0  0.0   0:00.40 kworker/0:0
...
```

##### 5.2.3 僵尸进程分析

> 僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，就需要找出父进程，然后在父进程里解决。

1. pstree

```bash
# -a 表示输出每个程序完整的命令（包含路径，参数或是常驻服务的标示）
# p指定PID
# s表示显示指定进程的父进程
$ pstree -aps 3084
systemd,1
  └─dockerd,15006 -H fd://
      └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3991 -namespace moby -workdir...
              └─app,4009
                  └─(app,3084)
```

2. 查看app-fix1.py代码

```python
int status = 0;
  for (;;) {
    for (int i = 0; i < 2; i++) {
      if(fork()== 0) {
        sub_process();
      }
    }
    sleep(5);
  }

  while(wait(&status)>0);
```

可以发现，文件错误地把 wait() 放到了 for 死循环的外面，也就是说，wait() 函数实际上并没被调用到，我们把它挪到 for 循环的里面就可以了。

修改后的文件我放到了 app-fix2.c ，运行对应的docker

```bash
# 先停止产生僵尸进程的 app
$ docker rm -f app
# 然后启动新的 app
$ docker run --privileged --name=app -itd feisky/app:iowait-fix2
```

3. top查看

   僵尸进程（Z 状态）没有了， iowait 也是 0，问题解决

```bash

$ top
top - 15:00:44 up 20 min,  1 user,  load average: 0.05, 0.05, 0.04
Tasks: 125 total,   1 running,  72 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  1.7 sy,  0.0 ni, 98.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  1.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3198 root      20   0    4376    840    780 S   0.3  0.0   0:00.01 app
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
    3 root      20   0       0      0      0 I   0.0  0.0   0:00.41 kworker/0:0
...
```



### 6 CPU软中断

> 中断是一种异步的事件处理机制，可以提高系统的并发处理能力
>
> 为了减少对正常进程运行调度的影响，中断处理程序应该尽快完成

#### 6.1 软中断

中断过程分为上半部和下半部：

* 上半部：用来快速处理中断，它在中断禁止模式下运行，主要处理和**硬件紧密相关**或者**时间敏感**的工作
* 下半部：用来延迟处理上半部未完成的工作，通常以**内核线程**的形式运行

网卡接收数据包的例子：网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。对上半部来说，既然是快速处理，其实就是要把网卡的数据**读到内存**中，然后**更新硬件寄存器的状态**（表示数据已经读好了），最后再发送一个软中断信号，通知下半部做进一步的处理。而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照**网络协议栈**，对数据进行**逐层解析和处理**，直到把它送给应用程序。

可以理解为：**上半部快速执行，下半部延迟执行**

#### 6.2 查看软中断和内核线程

1. 查看/proc文件系统

* /proc/softirqs，提供了软中断的运行情况
* /proc/interrupts，提供了硬中断的运行情况

```bash
# 可以看到各类软中断在不同CPU上累积的运行次数
$ cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:     811613    1972736
      NET_TX:         49          7
      NET_RX:    1136736    1506885
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     304787       3691
       SCHED:     689718    1897539
     HRTIMER:          0          0
         RCU:    1330771    1354737
```

**注意**：

* 软中断的类型：第一列的内容，对应软中断的类型，比如**NET_TX代表网络接收中断，NET_RX代表网络发送中断，SCHE代表调度，TIMER代表定时器**等等
* 每种软中断在不同CPU上的运行情况：同一行的内容，正常情况下，同一种中断在不同CPU上的累计次数应该差不多，比如NET_RX。 而TASKLET只在调用它的函数所在的CPU运行（存在的**问题**：由于只在一个 CPU 上运行导致的调度不均衡，或者因为不能在多个 CPU 上并行运行带来了性能限制）

2. 软中断以内核线程方式运行，每个CPU都对应一个软中断内核线程（ksoftirqd/CPU编号）

```bash
$ ps aux | grep softirq
root         7  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/0]
root        16  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/1]
```



### 7 系统的软中断CPU使用率升高

### 8 套路篇：如何迅速分析出CPU的瓶颈在哪里？

#### 8.1 CPU性能指标

性能指标总览

![img](https://static001.geekbang.org/resource/image/1e/07/1e66612e0022cd6c17847f3ab6989007.png)

##### 8.1.1 CPU使用率

CPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 

* CPU 使用率（nice），表示 CPU 在**用户态**运行的时间百分比。用户 CPU 使用率高，通常说明有**应用程序**比较繁忙。
* 系统 CPU 使用率，表示 CPU 在**内核态**运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明**内核**比较繁忙。
* 等待 I/O 的 CPU 使用率，通常也称为 **iowait**，表示**等待 I/O** 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。
* 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。
* 除了上面这些，还有在虚拟化环境中会用到的**窃取 CPU 使用率（steal）**和**客户 CPU 使用率（guest）**，分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。

##### 8.1.2 平均负载

> 系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载。
>
> 理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了。

##### 8.1.3 进程上下文切换

进程上下文切换分为：

1. 自愿上下文切换
2. 非自愿上下文切换

**注意**：过多的上下文切换，会将原本运行进程的 CPU 时间，消耗在**寄存器、内核栈以及虚拟内存等数据的保存和恢复**上，缩短进程真正运行的时间，成为性能瓶颈

##### 8.1.4 CPU缓存命中率

CPU 缓存的速度介于 CPU 和内存之间，缓存的是**热点的内存数据**。

如下图，根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中， L3 则用在多核中。从 L1 到 L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 **CPU 缓存的复用情况**，命中率越高，则表示性能越好。

![img](https://static001.geekbang.org/resource/image/aa/33/aa08816b60e453b52b5fae5e63549e33.png)

#### 8.2 CPU性能工具







## 内存性能篇

### 15 Linux内存怎么工作







## IO性能篇

### 23 Linux文件系统如何工作

#### 索引节点和目录项

* 索引节点，简称inode，和文件一一对应，存储在磁盘中，记录文件的元数据
* 目录项，dentry，记录文件的名字、索引节点以及其他目录项的关联关系



举例说明，为文件创建的硬链接，会对应不同的目录项，他们都连接到同一个文件，索引节点相同



磁盘的最小单位是**扇区**，文件系统将连续的扇区组成逻辑块，以逻辑块为最小单位，来读写磁盘数据。常见的逻辑块4KB，由连续的8个扇区组成。

**示意图**

![img](https://static001.geekbang.org/resource/image/32/47/328d942a38230a973f11bae67307be47.png)



磁盘在执行文件系统格式化时，分为三个区域：超级块、索引节点和数据块区

* 超级块：整个文件系统的状态
* 索引节点区：存储索引节点
* 数据块区：存储文件数据



#### 虚拟文件系统

**示意图**

![img](https://static001.geekbang.org/resource/image/72/12/728b7b39252a1e23a7a223cdf4aa1612.png)

文件系统分类：

* 基于磁盘的文件系统：常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统
* 基于内存的文件系统：常说的虚拟文件系统，不需要磁盘空间，但是占用内存。比如，/proc和/sys
* 网络文件系统：用来访问其他计算机的文件系统，比如NFS、SMB、iSCSI 等

**注意**：这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为**挂载点**），然后才能访问其中的文件。



#### 文件系统IO

根据是否利用标准库缓存，分为缓冲IO和非缓冲IO

* 缓存IO：利用标准库缓，加速文件访问，标准库内部利用系统调用访问文件
* 非缓存IO：直接通过系统调用访问文件，不再经过标准库缓存

**注意**：这里的“缓冲”，是指标准库内部实现的缓存，最终还是需要通过系统调用，而系统调用还会通过**页缓存**，来减少磁盘的IO操作



根据是否利用操作系统的页缓存，分为直接IO和非直接IO

* 直接IO：跳过操作系统的页缓存，直接和**文件系统**交互来访问文件
* 非直接IO：先通过页缓存，再通过内核或者额外的系统调用，真正和磁盘交互（`O_DIRECT`标志）



根据应用程序是否阻塞自身，分为阻塞IO和非阻塞IO



根据是否等待相应结果，分为同步IO和异步IO

* 同步IO：应用程序执行IO操作之后，要等到整个IO完成后，才能获得IO响应
* 异步IO：应用程序不用等待IO完成，会继续执行，等到IO执行完成，会以事件的方式通知应用程序

设置`O_SYNC`或者`O_DSYNC`，代表同步IO。如果是`O_DSYNC`，要等到文件数据写入磁盘之后，才能返回，如果是`O_SYNC`，是在`O_DSYNC`的基础上，要求文件**元数据**写入磁盘，才返回



设置`O_ASYNC`，代表异步IO，系统会再通过`SIGIO`或者`SIGPOLL`通知进程

#### 性能观测

##### 容量

`df`命令查看磁盘空间

```bash
$ df -h /dev/sda1 
Filesystem      Size  Used Avail Use% Mounted on 
/dev/sda1        29G  3.1G   26G  11% / 

# 查看索引节点所占的空间
$ df -i /dev/sda1 
Filesystem      Inodes  IUsed   IFree IUse% Mounted on 
/dev/sda1      3870720 157460 3713260    5% / 
```

当索引节点空间不足，但是磁盘空间充足时，很可能是过多小文件导致的。**解决方法**一般是删除这些小文件，或者移动到索引节点充足的其他磁盘区

##### 缓存

可以使用free或者vmstat，观察页缓存的大小

也可以查看/proc/meminfo

```bash
$ cat /proc/meminfo | grep -E "SReclaimable|Cached" 
Cached:           748316 kB 
SwapCached:            0 kB 
SReclaimable:     179508 kB 
```

内核使用slab机制，管理目录项和索引节点的缓存，/proc/meminfo给出了整体的slab大小，/proc/slabinfo可以查看每一种slab的缓存

```bash

$ cat /proc/slabinfo | grep -E '^#|dentry|inode' 
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail> 
xfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 
... 
ext4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 
sock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 
shmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 
proc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 
inode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 
dentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 
```

其中，dentry代表目录项缓存，inode_cache代表VFS索引节点缓存，其他的就是各种文件系统的索引节点缓存



实际性能分析中，更常使用slabtop命令，来找出占用内存最多的缓存类型

示例如下：可以看到，目录项和索引节点占用了最多的 Slab 缓存，总共大约23M

```bash

# 按下c按照缓存大小排序，按下a按照活跃对象数排序 
$ slabtop 
Active / Total Objects (% used)    : 277970 / 358914 (77.4%) 
Active / Total Slabs (% used)      : 12414 / 12414 (100.0%) 
Active / Total Caches (% used)     : 83 / 135 (61.5%) 
Active / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) 
Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K 

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME 
69804  23094   0%    0.19K   3324       21     13296K dentry 
16380  15854   0%    0.59K   1260       13     10080K inode_cache 
58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache 
   485    413   0%    5.69K     97        5      3104K task_struct 
  1472   1397   0%    2.00K     92       16      2944K kmalloc-2048 
```



## 问题记录

#### 19 swap使用升高

1. 文件页和匿名页的回收

#### 21 系统内存套路篇

1. cgroups等方式限制进程内存使用情况  ->   cgroup学习
2. 主缺页异常和次缺页异常

#### Linux文件系统怎么工作

1. 目录项、目录的区别（目录也是文件，可以用inode节点表示，而目录项在系统缓存中，如何构成目录结构？）

>  内存中的目录项指向目录对应的inode节点，目录inode中的数据指针指向磁盘中的数据块，数据块中包含着目录下的文件列表，从而映射到子目录下

